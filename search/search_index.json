{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DPKS (Data Processing Kitchen Sink) DPKS provides easily accesible data processing and explainable machine learning for omics data. Free software: MIT license Documentation: . Overview DPKS is a comprehensive python library for statistical analysis and explainable machine learning for omics data. DPKS allows for easily configurable and reproducible analysis pipelines that will simplify workflows and allow for exploration. Additionally, it exposes advances explainable machine learning techniques with a simple API allowing them to be used by non-machine learning practicioners in the field. An overview of DPKS and some of its main functionality. From the abstract from our preprint: Abstract The application of machine learning algorithms to facilitate the understanding of changes in proteome states has emerged as a promising methodology in proteomics research. Unfortunately, these methods can prove difficult to interpret, as it may not be immediately obvious how models reach their predictions. We present the data processing kitchen sink (DPKS) which provides reproducible access to classic statistical methods and advanced explainable machine learning algorithms to build highly accurate and fully interpretable predictive models. In DPKS, explainable machine learning methods are used to calculate the importance of each protein towards the prediction of a model for a particular proteome state. The calculated importance of each protein can enable the identification of proteins that drive phenotypic change in a data-driven manner while classic techniques rely on arbitrary cutoffs that may exclude important features from consideration. DPKS is a free and open source Python package available at https://github.com/InfectionMedicineProteomics/DPKS . 1 Example DPKS leverages method chaining, allowing for easily customizable pipelines to be created with minimal lines of code. Below is an example of how an analysis might be conducted by combining normalization, protein quantification, differential abundance analysis, explainable machine learning, and pathway enrichment analysis utilizing using DPKS. import xgboost from dpks.quant_matrix import QuantMatrix quant_data = \"quant_data.tsv\" design_matrix = \"design_matrix.tsv\" clf = xgboost.XGBClassifier( max_depth=2, reg_lambda=2, objective=\"binary:logistic\", seed=42 ) qm = ( QuantMatrix( quantification_file=quant_data, design_matrix_file=design_matrix ) .filter() .normalize( method=\"mean\", use_rt_sliding_window_filter=True ) .quantify( method=\"maxlfq\", threads=10, top_n=5 ) .compare( method=\"linregress\", min_samples_per_group=2, comparisons=[(2, 1), (3, 1)] ) .explain( clf, comparisons=[(2, 1), (3, 1)], n_iterations=100, downsample_background=True ) .annotate() ) enr = qm.enrich( method=\"overreptest\", libraries=['GO_Biological_Process_2023', 'KEGG_2021_Human', 'Reactome_2022'], filter_shap=True ) Here, we parse in the quantitative data and a design matrix, filter out contaminants and precursors below a 1% FDR threshold, normalize the data using the mean of a retention time sliding window filter, quantify proteins using relative quantification, perform differential abundance analysis using linear regression between 2 different groups of comparisons, explain those comparisons using explainable machine learning with 100 iterations of a downsampled bootstrap interpreter, and then annotate the uniprot IDs with their corresponding gene names. Finally, we take the output from the above analysis and perform pathway enrichment overrepresentation statistical tests using 3 different pathway databases on only proteins considered important during classification from the explain step. DPKS makes complicated analysis easy, and allows you to explore multiple analytical avenues in a clean and concise manner. Getting started Take a look at the Usage section for instructions on how to get started. Aaron M. Scott, Erik Hartman, Johan Malmstr\u00f6m, Lars Malmstr\u00f6m. Explainable machine learning for the identification of proteome states via the data processing kitchen sink. bioRxiv 2023.08.30.555506; doi: https://doi.org/10.1101/2023.08.30.555506 \u21a9","title":"Home"},{"location":"#dpks-data-processing-kitchen-sink","text":"DPKS provides easily accesible data processing and explainable machine learning for omics data. Free software: MIT license Documentation: .","title":"DPKS (Data Processing Kitchen Sink)"},{"location":"#overview","text":"DPKS is a comprehensive python library for statistical analysis and explainable machine learning for omics data. DPKS allows for easily configurable and reproducible analysis pipelines that will simplify workflows and allow for exploration. Additionally, it exposes advances explainable machine learning techniques with a simple API allowing them to be used by non-machine learning practicioners in the field. An overview of DPKS and some of its main functionality. From the abstract from our preprint: Abstract The application of machine learning algorithms to facilitate the understanding of changes in proteome states has emerged as a promising methodology in proteomics research. Unfortunately, these methods can prove difficult to interpret, as it may not be immediately obvious how models reach their predictions. We present the data processing kitchen sink (DPKS) which provides reproducible access to classic statistical methods and advanced explainable machine learning algorithms to build highly accurate and fully interpretable predictive models. In DPKS, explainable machine learning methods are used to calculate the importance of each protein towards the prediction of a model for a particular proteome state. The calculated importance of each protein can enable the identification of proteins that drive phenotypic change in a data-driven manner while classic techniques rely on arbitrary cutoffs that may exclude important features from consideration. DPKS is a free and open source Python package available at https://github.com/InfectionMedicineProteomics/DPKS . 1","title":"Overview"},{"location":"#example","text":"DPKS leverages method chaining, allowing for easily customizable pipelines to be created with minimal lines of code. Below is an example of how an analysis might be conducted by combining normalization, protein quantification, differential abundance analysis, explainable machine learning, and pathway enrichment analysis utilizing using DPKS. import xgboost from dpks.quant_matrix import QuantMatrix quant_data = \"quant_data.tsv\" design_matrix = \"design_matrix.tsv\" clf = xgboost.XGBClassifier( max_depth=2, reg_lambda=2, objective=\"binary:logistic\", seed=42 ) qm = ( QuantMatrix( quantification_file=quant_data, design_matrix_file=design_matrix ) .filter() .normalize( method=\"mean\", use_rt_sliding_window_filter=True ) .quantify( method=\"maxlfq\", threads=10, top_n=5 ) .compare( method=\"linregress\", min_samples_per_group=2, comparisons=[(2, 1), (3, 1)] ) .explain( clf, comparisons=[(2, 1), (3, 1)], n_iterations=100, downsample_background=True ) .annotate() ) enr = qm.enrich( method=\"overreptest\", libraries=['GO_Biological_Process_2023', 'KEGG_2021_Human', 'Reactome_2022'], filter_shap=True ) Here, we parse in the quantitative data and a design matrix, filter out contaminants and precursors below a 1% FDR threshold, normalize the data using the mean of a retention time sliding window filter, quantify proteins using relative quantification, perform differential abundance analysis using linear regression between 2 different groups of comparisons, explain those comparisons using explainable machine learning with 100 iterations of a downsampled bootstrap interpreter, and then annotate the uniprot IDs with their corresponding gene names. Finally, we take the output from the above analysis and perform pathway enrichment overrepresentation statistical tests using 3 different pathway databases on only proteins considered important during classification from the explain step. DPKS makes complicated analysis easy, and allows you to explore multiple analytical avenues in a clean and concise manner.","title":"Example"},{"location":"#getting-started","text":"Take a look at the Usage section for instructions on how to get started. Aaron M. Scott, Erik Hartman, Johan Malmstr\u00f6m, Lars Malmstr\u00f6m. Explainable machine learning for the identification of proteome states via the data processing kitchen sink. bioRxiv 2023.08.30.555506; doi: https://doi.org/10.1101/2023.08.30.555506 \u21a9","title":"Getting started"},{"location":"contributing/","text":"Contributing Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions Report Bugs Report bugs at . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs Look through the GitHub issues for bugs. Anything tagged with \\\"bug\\\" and \\\"help wanted\\\" is open to whoever wants to implement it. Implement Features Look through the GitHub issues for features. Anything tagged with \\\"enhancement\\\" and \\\"help wanted\\\" is open to whoever wants to implement it. Write Documentation dpks could always use more documentation, whether as part of the official dpks docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback The best way to send feedback is to file an issue at . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! Ready to contribute? Here\\'s how to set up dpks for local development. Fork the dpks repo on GitHub. Clone your fork locally: shell git clone git@github.com:your_name_here/DPKS.git Install your local copy into a virtualenv (created using conda or other tools): shell cd DPKS/ pip install -e . Create a branch for local development: shell git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you\\'re done making changes, check that your changes pass flake8 and the tests, including testing other Python versions with tox. Make sure to run black to format the code, also: shell flake8 dpks tests black dpks/ python setup.py test or pytest tox To get flake8, tox, and black, just pip install them into your virtualenv. Commit your changes and push your branch to GitHub: shell git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.rst. The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and for PyPy. Check https://travis-ci.com/arnscott/dpks/pull_requests and make sure that the tests pass for all supported Python versions. Tips To run a subset of tests: pytest tests.test_dpks Deploying A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.rst). Then run: bump2version patch # possible: major / minor / patch git push git push --tags Travis will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \\\"bug\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \\\"enhancement\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"dpks could always use more documentation, whether as part of the official dpks docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here\\'s how to set up dpks for local development. Fork the dpks repo on GitHub. Clone your fork locally: shell git clone git@github.com:your_name_here/DPKS.git Install your local copy into a virtualenv (created using conda or other tools): shell cd DPKS/ pip install -e . Create a branch for local development: shell git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you\\'re done making changes, check that your changes pass flake8 and the tests, including testing other Python versions with tox. Make sure to run black to format the code, also: shell flake8 dpks tests black dpks/ python setup.py test or pytest tox To get flake8, tox, and black, just pip install them into your virtualenv. Commit your changes and push your branch to GitHub: shell git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.rst. The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and for PyPy. Check https://travis-ci.com/arnscott/dpks/pull_requests and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"To run a subset of tests: pytest tests.test_dpks","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.rst). Then run: bump2version patch # possible: major / minor / patch git push git push --tags Travis will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"examples/","text":"Example Notebooks Notebooks There are notebooks with examples on how to use dpks in GitHub Notebooks ; the notebooks are listed in the table below. Quant Matrix : Demonstrates the basic use of the QuantMatrix. Start here. Differential Expression : Demonstrates how to compute differences between two experimental conditions. Explainable Machine Learning : Demonstrates how to compute feature importance for proteins when predicting some experimental conditions. Pathway Enrichment : Demonstrates how to perform pathway enrichment analysis. Overall Workflow : A concise demonstration of how to string together functionality in DPKS.","title":"Notebooks"},{"location":"examples/#example-notebooks","text":"","title":"Example Notebooks"},{"location":"examples/#notebooks","text":"There are notebooks with examples on how to use dpks in GitHub Notebooks ; the notebooks are listed in the table below. Quant Matrix : Demonstrates the basic use of the QuantMatrix. Start here. Differential Expression : Demonstrates how to compute differences between two experimental conditions. Explainable Machine Learning : Demonstrates how to compute feature importance for proteins when predicting some experimental conditions. Pathway Enrichment : Demonstrates how to perform pathway enrichment analysis. Overall Workflow : A concise demonstration of how to string together functionality in DPKS.","title":"Notebooks"},{"location":"reference/quant_matrix_ref/","text":"QuantMatrix Class for working with quantitative matrices. Source code in dpks/quant_matrix.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 class QuantMatrix : \"\"\"Class for working with quantitative matrices.\"\"\" quantification_file_path : Union [ str , pd . DataFrame ] design_matrix_file : Union [ str , pd . DataFrame ] num_rows : int num_samples : int quantitative_data : ad . AnnData explain_results : Optional [ list [ tuple [ Any , BootstrapInterpreter ]]] def __init__ ( self , quantification_file : Union [ str , pd . DataFrame ], design_matrix_file : Union [ str , pd . DataFrame ], annotation_fasta_file : str = None , quant_type : str = \"gps\" , diann_qvalue : float = 0.01 , ) -> None : \"\"\"Initialize the QuantMatrix instance. Args: quantification_file (Union[str, pd.DataFrame]): Path to the quantification file or DataFrame. design_matrix_file (Union[str, pd.DataFrame]): Path to the design matrix file or DataFrame. annotation_fasta_file (str, optional): Path to the annotation FASTA file. Defaults to None. quant_type (str, optional): Type of quantification. Defaults to \"gps\". diann_qvalue (float, optional): DIANN q-value. Defaults to 0.01. Examples: >>> quant_matrix = QuantMatrix(\"quantification.tsv\", \"design_matrix.csv\", annotation_fasta_file=\"annotation.fasta\") \"\"\" self . annotated = False self . explain_results = None if isinstance ( design_matrix_file , str ): design_matrix_file = pd . read_csv ( design_matrix_file , sep = \" \\t \" ) design_matrix_file . columns = map ( str . lower , design_matrix_file . columns ) if isinstance ( quantification_file , str ): if quant_type == \"gps\" : quantification_file = pd . read_csv ( quantification_file , sep = \" \\t \" ) elif quant_type == \"diann\" : quantification_file = parse_diann ( quantification_file , diann_qvalue ) else : if quant_type == \"diann\" : quantification_file = parse_diann ( quantification_file , diann_qvalue ) self . num_samples = len ( design_matrix_file ) self . num_rows = len ( quantification_file ) rt_column = \"\" if \"RT\" in quantification_file : rt_column = \"RT\" elif \"RetentionTime\" in quantification_file : rt_column = \"RetentionTime\" if rt_column : quantification_file = quantification_file . sort_values ( rt_column ) quantification_file = quantification_file . reset_index ( drop = True ) quantitative_data = ( quantification_file [ list ( design_matrix_file [ \"sample\" ])] . copy () . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str )) ) row_obs = quantification_file . drop ( list ( design_matrix_file [ \"sample\" ]), axis = 1 ) . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str )) if annotation_fasta_file is not None : row_obs [ \"ProteinLabel\" ] = get_protein_labels ( row_obs [ \"Protein\" ], annotation_fasta_file ) self . quantitative_data = ad . AnnData ( quantitative_data , obs = row_obs , var = design_matrix_file . copy () . set_index ( design_matrix_file [ \"sample\" ]), dtype = np . float64 , ) @property def proteins ( self ) -> List [ str ]: return list ( self . quantitative_data . obs [ \"Protein\" ] . unique ()) @property def protein_labels ( self ) -> List [ str ]: return self . row_annotations [ \"ProteinLabel\" ] . to_list () @property def sample_groups ( self ) -> List [ str ]: return self . sample_annotations [ \"group\" ] . to_list () @property def peptides ( self ) -> List [ str ]: return list ( self . quantitative_data . obs [ \"PeptideSequence\" ] . unique ()) @property def precursors ( self ) -> List [ str ]: self . row_annotations [ \"PrecursorId\" ] = ( self . row_annotations [ \"PeptideSequence\" ] + \"_\" + self . row_annotations [ \"Charge\" ] . astype ( str ) ) return list ( self . row_annotations [ \"PrecursorId\" ] . unique ()) @property def sample_annotations ( self ) -> pd . DataFrame : return self . quantitative_data . var @property def row_annotations ( self ) -> pd . DataFrame : return self . quantitative_data . obs @row_annotations . setter def row_annotations ( self , value : pd . DataFrame ) -> None : self . quantitative_data . obs = value def get_samples ( self , group : int ) -> List [ str ]: return list ( self . sample_annotations [ self . sample_annotations [ \"group\" ] == group ][ \"sample\" ] ) def get_pairs ( self , samples : list ) -> List [ str ]: sorted_samples = ( self . sample_annotations [ self . sample_annotations [ \"sample\" ] . isin ( samples )] . set_index ( \"sample\" ) . loc [ samples ] ) return list ( sorted_samples [ \"pair\" ]) def filter ( self , peptide_q_value : float = 0.01 , protein_q_value : float = 0.01 , remove_decoys : bool = True , remove_contaminants : bool = True , remove_non_proteotypic : bool = True , ) -> QuantMatrix : \"\"\"Filter the QuantMatrix. Args: peptide_q_value (float, optional): Peptide q-value threshold. Defaults to 0.01. protein_q_value (float, optional): Protein q-value threshold. Defaults to 0.01. remove_decoys (bool, optional): Whether to remove decoy entries. Defaults to True. remove_contaminants (bool, optional): Whether to remove contaminant entries. Defaults to True. remove_non_proteotypic (bool, optional): Whether to remove non-proteotypic entries. Defaults to True. Returns: QuantMatrix: Filtered QuantMatrix object. Examples: >>> print(quant_matrix.to_df().shape) (16679, 26) >>> print(quant_matrix.filter(peptide_q_value=0.001).to_df().shape) (15355, 26) \"\"\" filtered_data = self . quantitative_data if \"PeptideQValue\" in self . quantitative_data . obs : filtered_data = self . quantitative_data [ ( self . quantitative_data . obs [ \"PeptideQValue\" ] <= peptide_q_value ) ] . copy () if \"ProteinQValue\" in self . quantitative_data . obs : filtered_data = self . quantitative_data [ ( self . quantitative_data . obs [ \"ProteinQValue\" ] <= protein_q_value ) ] . copy () if remove_decoys : if \"Decoy\" in filtered_data . obs : filtered_data = filtered_data [ filtered_data . obs [ \"Decoy\" ] == 0 ] . copy () if remove_contaminants : filtered_data = filtered_data [ ~ filtered_data . obs [ \"Protein\" ] . str . contains ( \"contam\" ) ] . copy () filtered_data = filtered_data [ ~ filtered_data . obs [ \"Protein\" ] . str . contains ( \"cont_crap\" ) ] . copy () if remove_non_proteotypic : filtered_data = filtered_data [ ~ filtered_data . obs [ \"Protein\" ] . str . contains ( \";\" ) ] . copy () self . num_rows = len ( filtered_data ) quantitative_data = ( filtered_data . to_df ()[ list ( filtered_data . var [ \"sample\" ])] . copy () . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str )) ) row_obs = filtered_data . obs . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str ) ) self . quantitative_data = ad . AnnData ( quantitative_data , obs = row_obs , var = filtered_data . var , dtype = np . float64 , ) return self def scale ( self , method : str , ) -> QuantMatrix : \"\"\"Scale the QuantMatrix data at the feature level (i.e Precursor or Protein). Args: method (str): Scaling method. Options are 'zscore', 'minmax', or 'absmax'. Returns: QuantMatrix: Scaled QuantMatrix object. Raises: ValueError: If the provided scaling method is not supported. \"\"\" base_method : ScalingMethod = ScalingMethod () if method == \"zscore\" : base_method = ZScoreScaling () elif method == \"minmax\" : base_method = MinMaxScaling () elif method == \"absmax\" : base_method = AbsMaxScaling () else : raise ValueError ( f \"Unsupported scaling method: { method } \" ) self . quantitative_data . X = base_method . fit_transform ( self . quantitative_data . X ) return self def normalize ( self , method : str , log_transform : bool = True , use_rt_sliding_window_filter : bool = False , ** kwargs : Union [ int , bool , str ], ) -> QuantMatrix : \"\"\"Normalize the QuantMatrix data. Args: method (str): Normalization method. Options are 'tic', 'median', or 'mean'. log_transform (bool, optional): Whether to log-transform the data. Defaults to True. use_rt_sliding_window_filter (bool, optional): Whether to use a sliding window filter. Defaults to False. Can only use if a RetentionTime column was loaded in the QuantMatrix **kwargs: Additional keyword arguments depending on the chosen method. Returns: QuantMatrix: Normalized QuantMatrix object. Raises: ValueError: If the provided normalization method is not supported. Examples: >>> quant_matrix.normalize(method=\"mean\") \"\"\" base_method : NormalizationMethod = NormalizationMethod () if method == \"tic\" : base_method = TicNormalization () elif method == \"median\" : base_method = MedianNormalization () elif method == \"mean\" : base_method = MeanNormalization () else : raise ValueError ( f \"Unsupported normalization method: { method } \" ) if use_rt_sliding_window_filter : minimum_data_points = int ( kwargs . get ( \"minimum_data_points\" , 100 )) stride = int ( kwargs . get ( \"stride\" , 1 )) use_overlapping_windows = bool ( kwargs . get ( \"use_overlapping_windows\" , True )) rt_unit = str ( kwargs . get ( \"rt_unit\" , \"minute\" )) rt_window_normalization = RTSlidingWindowNormalization ( base_method = base_method , minimum_data_points = minimum_data_points , stride = stride , use_overlapping_windows = use_overlapping_windows , rt_unit = rt_unit , ) self . quantitative_data . X = rt_window_normalization . fit_transform ( self ) else : self . quantitative_data . X = base_method . fit_transform ( self . quantitative_data . X ) if log_transform : self . quantitative_data . X = Log2Normalization () . fit_transform ( self . quantitative_data . X ) return self def quantify ( self , method : str , ** kwargs : Union [ int , str ], ) -> QuantMatrix : \"\"\"Calculate protein quantities. Args: method (str): Quantification method. Options are 'top_n' or 'maxlfq'. **kwargs: Additional keyword arguments depending on the chosen method. Returns: QuantMatrix: Quantified protein matrix. Raises: ValueError: If the provided quantification method is not supported. Examples: >>> quant_matrix.quantify(method=\"top_n\", top_n=1) \"\"\" if method == \"top_n\" : level = str ( kwargs . get ( \"level\" , \"protein\" )) top_n = int ( kwargs . get ( \"top_n\" , 1 )) summarization_method = str ( kwargs . get ( \"summarization_method\" , \"sum\" )) quantifications = TopN ( top_n = top_n , level = level , summarization_method = summarization_method ) . quantify ( self ) design_matrix = self . quantitative_data . var protein_quantifications = QuantMatrix ( quantifications , design_matrix_file = design_matrix ) elif method == \"maxlfq\" : level = str ( kwargs . get ( \"level\" , \"protein\" )) threads = int ( kwargs . get ( \"threads\" , 1 )) minimum_subgroups = int ( kwargs . get ( \"minimum_subgroups\" , 1 )) top_n = int ( kwargs . get ( \"top_n\" , 0 )) quantifications = MaxLFQ ( level = level , threads = threads , minimum_subgroups = minimum_subgroups , top_n = top_n , ) . quantify ( self ) design_matrix = self . quantitative_data . var protein_quantifications = QuantMatrix ( quantifications , design_matrix_file = design_matrix ) else : raise ValueError ( f \"Unsupported quantification method: { method } \" ) return protein_quantifications def impute ( self , method : str , ** kwargs : int ) -> QuantMatrix : \"\"\"Impute missing values in the quantitative data. Args: method (str): The imputation method to use. Options are \"uniform_percentile\" and \"uniform_range\" **kwargs (int): Additional keyword arguments specific to the imputation method. Returns: QuantMatrix: The QuantMatrix object with missing values imputed. Raises: ValueError: If an unsupported imputation method is provided. Examples: >>> quant_matrix.impute(method=\"uniform_percentile\", percentile=0.1) \"\"\" base_method : ImputerMethod = ImputerMethod () if method == \"uniform_percentile\" : percentile = float ( kwargs . get ( \"percentile\" , 0.1 )) base_method = UniformPercentileImputer ( percentile = percentile ) elif method == \"uniform_range\" : maxvalue = int ( kwargs . get ( \"maxvalue\" , 1 )) minvalue = int ( kwargs . get ( \"minvalue\" , 0 )) base_method = UniformRangeImputer ( maxvalue = maxvalue , minvalue = minvalue ) else : raise ValueError ( f \"Unsupported imputation method: { method } \" ) self . quantitative_data . X = base_method . fit_transform ( self . quantitative_data . X ) return self def compare ( self , method : str , comparisons : list , min_samples_per_group : int = 2 , level : str = \"protein\" , multiple_testing_correction_method : str = \"fdr_tsbh\" , ) -> QuantMatrix : \"\"\"Compare groups by differential testing. Args: method (str): Statistical comparison method. Options are 'ttest', 'linregress', 'anova', 'ttest_paired'. comparisons (list): List of tuples specifying the group comparisons. min_samples_per_group (int, optional): Minimum number of samples per group. Defaults to 2. level (str, optional): Level of comparison. Defaults to 'protein'. multiple_testing_correction_method (str, optional): Method for multiple testing correction. Defaults to 'fdr_tsbh'. Returns: QuantMatrix: Matrix containing the results of the differential testing. Raises: ValueError: If the provided statistical comparison method is not supported. Examples: >>> quantified_data = quantified_data.compare( >>> method=\"linregress\", >>> min_samples_per_group=2, >>> comparisons=[(2, 1), (3, 1)] >>> ) \"\"\" if not method in { 'ttest' , 'linregress' , 'anova' , 'ttest_paired' }: raise ValueError ( f \"Unsupported statistical comparison method: { method } \" ) differential_test = DifferentialTest ( method , comparisons , min_samples_per_group , level , multiple_testing_correction_method , ) compared_data = differential_test . test ( self ) self . row_annotations = compared_data . row_annotations . copy () return self def explain ( self , clf , comparisons : list , n_iterations : int = 100 , downsample_background : bool = True , feature_column : str = \"Protein\" , ) -> QuantMatrix : \"\"\"Explain group differences using explainable machine learning and feature importance. Args: clf: Classifier object used for prediction. comparisons (list): List of tuples specifying the group comparisons. n_iterations (int, optional): Number of iterations for bootstrapping. Defaults to 100. downsample_background (bool, optional): Whether to downsample the background. Defaults to True. feature_column (str, optional): Name of the feature column. Defaults to 'Protein'. Returns: QuantMatrix: Matrix containing the results of the explanation. Examples: >>> import xgboost >>> >>> clf = xgboost.XGBClassifier( >>> max_depth=2, >>> reg_lambda=2, >>> objective=\"binary:logistic\", >>> seed=42 >>> ) >>> >>> quantified_data = quantified_data.explain( >>> clf, >>> comparisons=[(2, 1), (3, 1)], >>> n_iterations=10, >>> downsample_background=True >>> ) \"\"\" explain_results = [] for comparison in comparisons : X , y = self . to_ml ( feature_column = feature_column , comparison = comparison ) scaler = StandardScaler () X [:] = scaler . fit_transform ( X [:]) clf_ = Classifier ( clf ) interpreter = BootstrapInterpreter ( feature_names = X . columns , n_iterations = n_iterations , downsample_background = downsample_background , ) interpreter . fit ( X , y , clf_ ) explain_results . append (( comparison , interpreter )) importances_df = interpreter . importances [ [ \"feature\" , \"mean_shap\" , \"mean_rank\" ] ] . set_index ( \"feature\" ) importances_df = importances_df . rename ( columns = { \"mean_shap\" : f \"MeanSHAP { comparison [ 0 ] } - { comparison [ 1 ] } \" , \"mean_rank\" : f \"MeanRank { comparison [ 0 ] } - { comparison [ 1 ] } \" , } ) self . row_annotations = self . row_annotations . join ( importances_df , on = \"Protein\" ) self . explain_results = explain_results return self def enrich ( self , method : str = \"overreptest\" , libraries : Optional [ list [ str ]] = None , organism : str = \"human\" , background : Optional [ Union [ list [ str ], str ]] = None , filter_pvalue : bool = False , pvalue_cutoff : float = 0.1 , pvalue_column : str = \"CorrectedPValue2-1\" , filter_shap : bool = False , shap_cutoff : float = 0.0 , shap_column : str = \"MeanSHAP2-1\" , subset_library : bool = False , ): \"\"\"Perform gene set enrichment analysis. Args: method (str, optional): Enrichment method to use. Options are \"enrichr_overreptest\" and \"overreptest\". Defaults to \"overreptest\". libraries (Optional[List[str]], optional): List of gene set libraries. Defaults to None. organism (str, optional): Organism for the analysis. Defaults to \"human\". background (Optional[Union[List[str], str]], optional): Background gene set. Defaults to None. filter_pvalue (bool, optional): Whether to filter by p-value. Defaults to False. pvalue_cutoff (float, optional): P-value cutoff for filtering. Defaults to 0.1. pvalue_column (str, optional): Column name for p-values. Defaults to \"CorrectedPValue2-1\". filter_shap (bool, optional): Whether to filter by SHAP value. Defaults to False. shap_cutoff (float, optional): SHAP value cutoff for filtering. Defaults to 0.0. shap_column (str, optional): Column name for SHAP values. Defaults to \"MeanSHAP2-1\". subset_library (bool, optional): Whether to subset the library. Defaults to False. Returns: Any: Enrichment result. Raises: ValueError: If the method is not supported. Examples: >>> enr = quantified_data.enrich( >>> method=\"enrichr_overreptest\", >>> filter_pvalue=True, >>> pvalue_column=\"CorrectedPValue2-1\", >>> pvalue_cutoff=0.1 >>> ) \"\"\" if not self . annotated : self . annotate () if not libraries : libraries = [ \"GO_Biological_Process_2023\" ] gene_df = pd . DataFrame () if filter_pvalue : gene_df = self . row_annotations [ self . row_annotations [ pvalue_column ] < pvalue_cutoff ] if filter_shap : gene_df = self . row_annotations [ self . row_annotations [ shap_column ] > shap_cutoff ] genes = gene_df [ \"Gene\" ] . to_list () if subset_library : temp_libraries = [] for library in libraries : go_bp = gp . get_library ( name = library , organism = organism ) gene_set = set ( gene_df [ \"Gene\" ] . to_list ()) bio_process_subset = dict () for key , value in go_bp . items (): for gene in value : if gene in gene_set : bio_process_subset [ key ] = value temp_libraries . append ( bio_process_subset ) libraries = temp_libraries enr = None if method == \"overreptest\" : if background : enr = gp . enrich ( gene_list = genes , gene_sets = libraries , background = background , ) else : enr = gp . enrich ( gene_list = genes , gene_sets = libraries ) elif method == \"enrichr_overreptest\" : if background : enr = gp . enrichr ( gene_list = genes , gene_sets = libraries , organism = organism , background = background , ) else : enr = gp . enrichr ( gene_list = genes , gene_sets = libraries , organism = organism , ) else : raise ValueError ( f \"Unsupported pathway enrichment method: { method } \" ) return enr def annotate ( self ): \"\"\"Annotate proteins with gene names. Returns: QuantMatrix: The annotated QuantMatrix object. Examples: >>> quant_matrix.annotate() \"\"\" request = IdMappingClient . submit ( source = \"UniProtKB_AC-ID\" , dest = \"Gene_Name\" , ids = self . proteins ) while True : status = request . get_status () if status in { \"FINISHED\" , \"ERROR\" }: break else : time . sleep ( 1 ) translation_result = list ( request . each_result ()) id_mapping = dict () for id_result in translation_result : mapping = id_mapping . get ( id_result [ \"from\" ], []) mapping . append ( id_result [ \"to\" ]) id_mapping [ id_result [ \"from\" ]] = mapping final_mapping = dict () for key , value in id_mapping . items (): value = value [ 0 ] final_mapping [ key ] = value mapping_df = pd . DataFrame ( { \"Protein\" : final_mapping . keys (), \"Gene\" : final_mapping . values ()} ) self . row_annotations = self . row_annotations . join ( mapping_df . set_index ( \"Protein\" ), on = \"Protein\" , how = \"left\" ) self . row_annotations [ \"Gene\" ] = self . row_annotations [ \"Gene\" ] . fillna ( self . row_annotations [ \"Protein\" ] ) self . annotated = True return self def predict ( self , classifier , scaler : Any = None , scale : bool = True , ) -> QuantMatrix : \"\"\"Predict labels using a classifier. Args: classifier: The classifier model to use for prediction. scaler (optional): The scaler object to use for data scaling. scale (bool): Whether to scale the data before prediction. Defaults to True. Returns: QuantMatrix: The QuantMatrix object with predicted labels. Examples: >>> quant_matrix.predict(classifier=clf, scaler=std_scaler) \"\"\" X = format_data ( self ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) classifier = Classifier ( classifier = classifier ) self . sample_annotations [ \"Prediction\" ] = classifier . predict ( X ) return self def interpret ( self , classifier , scaler : Any = None , shap_algorithm : str = \"auto\" , scale : bool = True , downsample_background = False , ) -> QuantMatrix : \"\"\"Interpret the model's predictions using SHAP values. Args: classifier: The classifier model to interpret. scaler (optional): The scaler object to use for data scaling. shap_algorithm (str): The SHAP algorithm to use. Defaults to \"auto\". scale (bool): Whether to scale the data before interpretation. Defaults to True. downsample_background (bool): Whether to downsample background data. Defaults to False. Returns: QuantMatrix: The QuantMatrix object with SHAP values added to observations. Examples: >>> quant_matrix.interpret(classifier=clf, scaler=std_scaler) \"\"\" X = format_data ( self ) y = encode_labels ( self . quantitative_data . var [ \"group\" ] . values ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) classifier = Classifier ( classifier = classifier , shap_algorithm = shap_algorithm ) if downsample_background : rus = RandomUnderSampler ( random_state = 0 ) X_resampled , y_resampled = rus . fit_resample ( X , y ) classifier . interpret ( X_resampled ) self . transformed_data = X_resampled self . y_resampled = y_resampled else : classifier . interpret ( X ) self . transformed_data = X self . classifier = classifier shap_values = classifier . feature_importances_ . tolist () self . quantitative_data . obs [ \"SHAP\" ] = shap_values self . shap = classifier . shap_values return self def train ( self , classifier , scaler : Any = None , scale : bool = True , validate : bool = True , scoring : str = \"accuracy\" , num_folds : int = 3 , random_state : int = 42 , shuffle : bool = False , ) -> TrainResult : \"\"\"Train a classifier on the quantitative data. Args: classifier: The classifier object or class to use for training. scaler (Any): The scaler object to scale the data. Defaults to None. scale (bool): Whether to scale the data. Defaults to True. validate (bool): Whether to perform cross-validation. Defaults to True. scoring (str): The scoring metric for cross-validation. Defaults to \"accuracy\". num_folds (int): The number of folds for cross-validation. Defaults to 3. random_state (int): Random seed for reproducibility. Defaults to 42. shuffle (bool): Whether to shuffle the data before splitting in cross-validation. Defaults to False. Returns: TrainResult: The result of the training process, including the trained classifier, scaler, and validation scores. Examples: >>> result = quant_matrix.train(classifier=RandomForestClassifier(), validate=True) \"\"\" X = format_data ( self ) y = encode_labels ( self . quantitative_data . var [ \"group\" ] . values ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) classifier = Classifier ( classifier = classifier ) validation_result = np . array ([]) if validate : cv = StratifiedKFold ( num_folds , shuffle = shuffle , random_state = random_state ) validation_result = cross_val_score ( classifier , X , y , scoring = scoring , cv = cv ) classifier . fit ( X , y ) return TrainResult ( classifier , scaler , validation_result ) def optimize ( self , classifier , param_search_method : str , param_grid : dict , scaler : Any = None , scale : bool = True , threads : int = 1 , random_state : int = 42 , folds : int = 3 , verbose : Union [ bool , int ] = False , ** kwargs : Union [ dict , int , str , bool ], ) -> ParamSearchResult : \"\"\"Optimize hyperparameters of a classifier using different search methods. Args: classifier: The classifier object or class to optimize. param_search_method (str): The parameter search method to use (\"genetic\" or \"random\"). param_grid (dict): The parameter grid to search over. scaler (Any): The scaler object to scale the data. Defaults to None. scale (bool): Whether to scale the data. Defaults to True. threads (int): The number of threads to use for optimization. Defaults to 1. random_state (int): Random seed for reproducibility. Defaults to 42. folds (int): The number of folds for cross-validation. Defaults to 3. verbose (Union[bool, int]): Verbosity level. Defaults to False. **kwargs: Additional keyword arguments specific to each search method. Returns: ParamSearchResult: The result of the parameter search, including the best estimator and parameter populations. Examples: >>> param_grid = {'max_depth': [3, 5, 7], 'min_samples_split': [2, 5, 10]} >>> result = quant_matrix.optimize(classifier=DecisionTreeClassifier(), param_search_method='random', param_grid=param_grid, verbose=True) >>> result.best_estimator_ DecisionTreeClassifier(max_depth=5, min_samples_split=10) \"\"\" X = format_data ( self ) y = encode_labels ( self . quantitative_data . var [ \"group\" ] . values ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) result = None if param_search_method == \"genetic\" : gas = GeneticAlgorithmSearch ( classifier , param_grid = param_grid , threads = threads , folds = folds , n_survive = kwargs . get ( \"n_survive\" , 5 ), pop_size = kwargs . get ( \"pop_size\" , 10 ), n_generations = kwargs . get ( \"n_generations\" , 20 ), verbose = verbose , random_state = kwargs . get ( \"random_state\" , None ), shuffle = kwargs . get ( \"shuffle\" , False ), ) parameter_populations = gas . fit ( X , y ) result = ParamSearchResult ( classifier = gas . best_estimator_ , result = parameter_populations , ) elif param_search_method == \"random\" : randomized_search = RandomizedSearch ( classifier , param_grid = param_grid , folds = folds , random_state = random_state , n_iter = kwargs . get ( \"n_iter\" , 30 ), n_jobs = threads , scoring = kwargs . get ( \"scoring\" , \"accuracy\" ), verbose = verbose , ) result = randomized_search . fit ( X , y ) return result def plot ( self , plot_type : str , save : bool = False , fig : matplotlib . figure . Figure = None , ax : Union [ list , matplotlib . axes . Axes ] = None , ** kwargs : Union [ np . ndarray , int , list , str , ], ) -> tuple [ matplotlib . figure . Figure , matplotlib . axes . Axes ]: \"\"\"Generate plots based on specified plot type. Args: plot_type (str): The type of plot to generate. Possible values are: - \"shap_summary\": SHAP summary plot. - \"rfe_pca\": Recursive Feature Elimination (RFE) with Principal Component Analysis (PCA) plot. save (bool): Whether to save the plot. Defaults to False. fig (matplotlib.figure.Figure): The matplotlib figure object. Defaults to None. ax (Union[list, matplotlib.axes.Axes]): The list of matplotlib axes objects or a single axes object. Defaults to None. **kwargs: Additional keyword arguments specific to each plot type. Returns: tuple[matplotlib.figure.Figure, matplotlib.axes.Axes]: The matplotlib figure and axes objects. Raises: ValueError: If an unsupported plot type is provided. Examples: >>> fig, ax = quant_matrix.plot(plot_type='shap_summary', save=True, n_display=10) \"\"\" if plot_type == \"shap_summary\" : try : getattr ( self , \"shap\" ) except AttributeError : print ( \"SHAP values have not been generated\" ) cmap = kwargs . get ( \"cmap\" , [ \"#ff4800\" , \"#ff4040\" , \"#a836ff\" , \"#405cff\" , \"#05c9fa\" , ], ) order_by = kwargs . get ( \"order_by\" , \"shap\" ) fig , ax = SHAPPlot ( fig = fig , ax = ax , shap_values = self . shap , X = self . transformed_data , qm = self , cmap = cmap , n_display = kwargs . get ( \"n_display\" , 5 ), jitter = kwargs . get ( \"jitter\" , 0.1 ), alpha = kwargs . get ( \"alpha\" , 0.75 ), n_bins = kwargs . get ( \"n_bins\" , 100 ), feature_column = kwargs . get ( \"feature_column\" , \"Protein\" ), order_by = order_by , ) . plot () if plot_type == \"rfe_pca\" : cmap = kwargs . get ( \"cmap\" , \"coolwarm\" ) cutoffs = list ( kwargs . get ( \"cutoffs\" , [ 100 , 50 , 10 ])) fig , ax = RFEPCA ( fig = fig , axs = ax , qm = self , cutoffs = cutoffs , cmap = cmap ) . plot () if save : filepath = str ( kwargs . get ( \"filepath\" , f \" { plot_type } .png\" )) dpi = int ( kwargs . get ( \"dpi\" , 300 )) matplotlib . pyplot . savefig ( filepath , dpi = dpi ) return fig , ax def detect ( self ) -> None : \"\"\"Not implemented Detect outliers in the samples \"\"\" pass def write ( self , file_path : str ) -> None : \"\"\"Write the QuantMatrix to a tab-separated file. Args: file_path (str): The path where the file will be saved. Returns: None Examples: >>> filename = \"protein.tsv\" >>> quant_matrix.write(filename) \"\"\" self . to_df () . to_csv ( file_path , sep = \" \\t \" , index = False ) def to_df ( self ) -> pd . DataFrame : \"\"\"Convert the QuantMatrix object to a pandas DataFrame. Returns: pd.DataFrame: DataFrame representation of the QuantMatrix. Examples: >>> quant_matrix.to_df() \"\"\" quant_data = self . quantitative_data [ self . row_annotations . index , :] . to_df () merged = pd . concat ([ self . row_annotations , quant_data ], axis = 1 ) return merged def to_ml ( self , feature_column : str = \"Protein\" , label_column : str = \"group\" , comparison : tuple = ( 1 , 2 ), ) -> tuple [ Any , Any ]: \"\"\"Converts the QuantMatrix object to features and labels for machine learning. Args: feature_column (str, optional): The column to use as features. Defaults to \"Protein\". label_column (str, optional): The column to use as labels. Defaults to \"group\". comparison (tuple, optional): The comparison groups. Defaults to (1, 2). Returns: tuple[Any, Any]: A tuple containing features and labels. Examples: >>> features, labels = quant_matrix.to_ml() \"\"\" qm_df = self . to_df () samples = self . sample_annotations [ self . sample_annotations [ \"group\" ] . isin ( comparison ) ][ \"sample\" ] . to_list () transposed_features = qm_df . set_index ( feature_column )[ samples ] . T sample_annotations = self . sample_annotations . copy () sample_annotations_subset = sample_annotations [ sample_annotations [ label_column ] . isin ( comparison ) ] . copy () encoder = LabelEncoder () sample_annotations_subset [ \"label\" ] = encoder . fit_transform ( sample_annotations_subset [ label_column ] ) combined = transposed_features . join ( sample_annotations_subset [[ \"sample\" , \"label\" ]] . set_index ( \"sample\" ), how = \"left\" , ) return combined . loc [:, combined . columns != \"label\" ], combined [[ \"label\" ]] __init__ ( quantification_file , design_matrix_file , annotation_fasta_file = None , quant_type = 'gps' , diann_qvalue = 0.01 ) Initialize the QuantMatrix instance. Parameters: quantification_file ( Union [ str , DataFrame ] ) \u2013 Path to the quantification file or DataFrame. design_matrix_file ( Union [ str , DataFrame ] ) \u2013 Path to the design matrix file or DataFrame. annotation_fasta_file ( str , default: None ) \u2013 Path to the annotation FASTA file. Defaults to None. quant_type ( str , default: 'gps' ) \u2013 Type of quantification. Defaults to \"gps\". diann_qvalue ( float , default: 0.01 ) \u2013 DIANN q-value. Defaults to 0.01. Examples: >>> quant_matrix = QuantMatrix ( \"quantification.tsv\" , \"design_matrix.csv\" , annotation_fasta_file = \"annotation.fasta\" ) Source code in dpks/quant_matrix.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def __init__ ( self , quantification_file : Union [ str , pd . DataFrame ], design_matrix_file : Union [ str , pd . DataFrame ], annotation_fasta_file : str = None , quant_type : str = \"gps\" , diann_qvalue : float = 0.01 , ) -> None : \"\"\"Initialize the QuantMatrix instance. Args: quantification_file (Union[str, pd.DataFrame]): Path to the quantification file or DataFrame. design_matrix_file (Union[str, pd.DataFrame]): Path to the design matrix file or DataFrame. annotation_fasta_file (str, optional): Path to the annotation FASTA file. Defaults to None. quant_type (str, optional): Type of quantification. Defaults to \"gps\". diann_qvalue (float, optional): DIANN q-value. Defaults to 0.01. Examples: >>> quant_matrix = QuantMatrix(\"quantification.tsv\", \"design_matrix.csv\", annotation_fasta_file=\"annotation.fasta\") \"\"\" self . annotated = False self . explain_results = None if isinstance ( design_matrix_file , str ): design_matrix_file = pd . read_csv ( design_matrix_file , sep = \" \\t \" ) design_matrix_file . columns = map ( str . lower , design_matrix_file . columns ) if isinstance ( quantification_file , str ): if quant_type == \"gps\" : quantification_file = pd . read_csv ( quantification_file , sep = \" \\t \" ) elif quant_type == \"diann\" : quantification_file = parse_diann ( quantification_file , diann_qvalue ) else : if quant_type == \"diann\" : quantification_file = parse_diann ( quantification_file , diann_qvalue ) self . num_samples = len ( design_matrix_file ) self . num_rows = len ( quantification_file ) rt_column = \"\" if \"RT\" in quantification_file : rt_column = \"RT\" elif \"RetentionTime\" in quantification_file : rt_column = \"RetentionTime\" if rt_column : quantification_file = quantification_file . sort_values ( rt_column ) quantification_file = quantification_file . reset_index ( drop = True ) quantitative_data = ( quantification_file [ list ( design_matrix_file [ \"sample\" ])] . copy () . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str )) ) row_obs = quantification_file . drop ( list ( design_matrix_file [ \"sample\" ]), axis = 1 ) . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str )) if annotation_fasta_file is not None : row_obs [ \"ProteinLabel\" ] = get_protein_labels ( row_obs [ \"Protein\" ], annotation_fasta_file ) self . quantitative_data = ad . AnnData ( quantitative_data , obs = row_obs , var = design_matrix_file . copy () . set_index ( design_matrix_file [ \"sample\" ]), dtype = np . float64 , ) annotate () Annotate proteins with gene names. Returns: QuantMatrix \u2013 The annotated QuantMatrix object. Examples: >>> quant_matrix . annotate () Source code in dpks/quant_matrix.py 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 def annotate ( self ): \"\"\"Annotate proteins with gene names. Returns: QuantMatrix: The annotated QuantMatrix object. Examples: >>> quant_matrix.annotate() \"\"\" request = IdMappingClient . submit ( source = \"UniProtKB_AC-ID\" , dest = \"Gene_Name\" , ids = self . proteins ) while True : status = request . get_status () if status in { \"FINISHED\" , \"ERROR\" }: break else : time . sleep ( 1 ) translation_result = list ( request . each_result ()) id_mapping = dict () for id_result in translation_result : mapping = id_mapping . get ( id_result [ \"from\" ], []) mapping . append ( id_result [ \"to\" ]) id_mapping [ id_result [ \"from\" ]] = mapping final_mapping = dict () for key , value in id_mapping . items (): value = value [ 0 ] final_mapping [ key ] = value mapping_df = pd . DataFrame ( { \"Protein\" : final_mapping . keys (), \"Gene\" : final_mapping . values ()} ) self . row_annotations = self . row_annotations . join ( mapping_df . set_index ( \"Protein\" ), on = \"Protein\" , how = \"left\" ) self . row_annotations [ \"Gene\" ] = self . row_annotations [ \"Gene\" ] . fillna ( self . row_annotations [ \"Protein\" ] ) self . annotated = True return self compare ( method , comparisons , min_samples_per_group = 2 , level = 'protein' , multiple_testing_correction_method = 'fdr_tsbh' ) Compare groups by differential testing. Parameters: method ( str ) \u2013 Statistical comparison method. Options are 'ttest', 'linregress', 'anova', 'ttest_paired'. comparisons ( list ) \u2013 List of tuples specifying the group comparisons. min_samples_per_group ( int , default: 2 ) \u2013 Minimum number of samples per group. Defaults to 2. level ( str , default: 'protein' ) \u2013 Level of comparison. Defaults to 'protein'. multiple_testing_correction_method ( str , default: 'fdr_tsbh' ) \u2013 Method for multiple testing correction. Defaults to 'fdr_tsbh'. Returns: QuantMatrix ( QuantMatrix ) \u2013 Matrix containing the results of the differential testing. Raises: ValueError \u2013 If the provided statistical comparison method is not supported. Examples: >>> quantified_data = quantified_data . compare ( >>> method = \"linregress\" , >>> min_samples_per_group = 2 , >>> comparisons = [( 2 , 1 ), ( 3 , 1 )] >>> ) Source code in dpks/quant_matrix.py 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 def compare ( self , method : str , comparisons : list , min_samples_per_group : int = 2 , level : str = \"protein\" , multiple_testing_correction_method : str = \"fdr_tsbh\" , ) -> QuantMatrix : \"\"\"Compare groups by differential testing. Args: method (str): Statistical comparison method. Options are 'ttest', 'linregress', 'anova', 'ttest_paired'. comparisons (list): List of tuples specifying the group comparisons. min_samples_per_group (int, optional): Minimum number of samples per group. Defaults to 2. level (str, optional): Level of comparison. Defaults to 'protein'. multiple_testing_correction_method (str, optional): Method for multiple testing correction. Defaults to 'fdr_tsbh'. Returns: QuantMatrix: Matrix containing the results of the differential testing. Raises: ValueError: If the provided statistical comparison method is not supported. Examples: >>> quantified_data = quantified_data.compare( >>> method=\"linregress\", >>> min_samples_per_group=2, >>> comparisons=[(2, 1), (3, 1)] >>> ) \"\"\" if not method in { 'ttest' , 'linregress' , 'anova' , 'ttest_paired' }: raise ValueError ( f \"Unsupported statistical comparison method: { method } \" ) differential_test = DifferentialTest ( method , comparisons , min_samples_per_group , level , multiple_testing_correction_method , ) compared_data = differential_test . test ( self ) self . row_annotations = compared_data . row_annotations . copy () return self detect () Not implemented Detect outliers in the samples Source code in dpks/quant_matrix.py 1106 1107 1108 1109 1110 1111 1112 def detect ( self ) -> None : \"\"\"Not implemented Detect outliers in the samples \"\"\" pass enrich ( method = 'overreptest' , libraries = None , organism = 'human' , background = None , filter_pvalue = False , pvalue_cutoff = 0.1 , pvalue_column = 'CorrectedPValue2-1' , filter_shap = False , shap_cutoff = 0.0 , shap_column = 'MeanSHAP2-1' , subset_library = False ) Perform gene set enrichment analysis. Parameters: method ( str , default: 'overreptest' ) \u2013 Enrichment method to use. Options are \"enrichr_overreptest\" and \"overreptest\". Defaults to \"overreptest\". libraries ( Optional [ List [ str ]] , default: None ) \u2013 List of gene set libraries. Defaults to None. organism ( str , default: 'human' ) \u2013 Organism for the analysis. Defaults to \"human\". background ( Optional [ Union [ List [ str ], str ]] , default: None ) \u2013 Background gene set. Defaults to None. filter_pvalue ( bool , default: False ) \u2013 Whether to filter by p-value. Defaults to False. pvalue_cutoff ( float , default: 0.1 ) \u2013 P-value cutoff for filtering. Defaults to 0.1. pvalue_column ( str , default: 'CorrectedPValue2-1' ) \u2013 Column name for p-values. Defaults to \"CorrectedPValue2-1\". filter_shap ( bool , default: False ) \u2013 Whether to filter by SHAP value. Defaults to False. shap_cutoff ( float , default: 0.0 ) \u2013 SHAP value cutoff for filtering. Defaults to 0.0. shap_column ( str , default: 'MeanSHAP2-1' ) \u2013 Column name for SHAP values. Defaults to \"MeanSHAP2-1\". subset_library ( bool , default: False ) \u2013 Whether to subset the library. Defaults to False. Returns: Any \u2013 Enrichment result. Raises: ValueError \u2013 If the method is not supported. Examples: >>> enr = quantified_data . enrich ( >>> method = \"enrichr_overreptest\" , >>> filter_pvalue = True , >>> pvalue_column = \"CorrectedPValue2-1\" , >>> pvalue_cutoff = 0.1 >>> ) Source code in dpks/quant_matrix.py 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 def enrich ( self , method : str = \"overreptest\" , libraries : Optional [ list [ str ]] = None , organism : str = \"human\" , background : Optional [ Union [ list [ str ], str ]] = None , filter_pvalue : bool = False , pvalue_cutoff : float = 0.1 , pvalue_column : str = \"CorrectedPValue2-1\" , filter_shap : bool = False , shap_cutoff : float = 0.0 , shap_column : str = \"MeanSHAP2-1\" , subset_library : bool = False , ): \"\"\"Perform gene set enrichment analysis. Args: method (str, optional): Enrichment method to use. Options are \"enrichr_overreptest\" and \"overreptest\". Defaults to \"overreptest\". libraries (Optional[List[str]], optional): List of gene set libraries. Defaults to None. organism (str, optional): Organism for the analysis. Defaults to \"human\". background (Optional[Union[List[str], str]], optional): Background gene set. Defaults to None. filter_pvalue (bool, optional): Whether to filter by p-value. Defaults to False. pvalue_cutoff (float, optional): P-value cutoff for filtering. Defaults to 0.1. pvalue_column (str, optional): Column name for p-values. Defaults to \"CorrectedPValue2-1\". filter_shap (bool, optional): Whether to filter by SHAP value. Defaults to False. shap_cutoff (float, optional): SHAP value cutoff for filtering. Defaults to 0.0. shap_column (str, optional): Column name for SHAP values. Defaults to \"MeanSHAP2-1\". subset_library (bool, optional): Whether to subset the library. Defaults to False. Returns: Any: Enrichment result. Raises: ValueError: If the method is not supported. Examples: >>> enr = quantified_data.enrich( >>> method=\"enrichr_overreptest\", >>> filter_pvalue=True, >>> pvalue_column=\"CorrectedPValue2-1\", >>> pvalue_cutoff=0.1 >>> ) \"\"\" if not self . annotated : self . annotate () if not libraries : libraries = [ \"GO_Biological_Process_2023\" ] gene_df = pd . DataFrame () if filter_pvalue : gene_df = self . row_annotations [ self . row_annotations [ pvalue_column ] < pvalue_cutoff ] if filter_shap : gene_df = self . row_annotations [ self . row_annotations [ shap_column ] > shap_cutoff ] genes = gene_df [ \"Gene\" ] . to_list () if subset_library : temp_libraries = [] for library in libraries : go_bp = gp . get_library ( name = library , organism = organism ) gene_set = set ( gene_df [ \"Gene\" ] . to_list ()) bio_process_subset = dict () for key , value in go_bp . items (): for gene in value : if gene in gene_set : bio_process_subset [ key ] = value temp_libraries . append ( bio_process_subset ) libraries = temp_libraries enr = None if method == \"overreptest\" : if background : enr = gp . enrich ( gene_list = genes , gene_sets = libraries , background = background , ) else : enr = gp . enrich ( gene_list = genes , gene_sets = libraries ) elif method == \"enrichr_overreptest\" : if background : enr = gp . enrichr ( gene_list = genes , gene_sets = libraries , organism = organism , background = background , ) else : enr = gp . enrichr ( gene_list = genes , gene_sets = libraries , organism = organism , ) else : raise ValueError ( f \"Unsupported pathway enrichment method: { method } \" ) return enr explain ( clf , comparisons , n_iterations = 100 , downsample_background = True , feature_column = 'Protein' ) Explain group differences using explainable machine learning and feature importance. Parameters: clf \u2013 Classifier object used for prediction. comparisons ( list ) \u2013 List of tuples specifying the group comparisons. n_iterations ( int , default: 100 ) \u2013 Number of iterations for bootstrapping. Defaults to 100. downsample_background ( bool , default: True ) \u2013 Whether to downsample the background. Defaults to True. feature_column ( str , default: 'Protein' ) \u2013 Name of the feature column. Defaults to 'Protein'. Returns: QuantMatrix ( QuantMatrix ) \u2013 Matrix containing the results of the explanation. Examples: >>> import xgboost >>> >>> clf = xgboost . XGBClassifier ( >>> max_depth = 2 , >>> reg_lambda = 2 , >>> objective = \"binary:logistic\" , >>> seed = 42 >>> ) >>> >>> quantified_data = quantified_data . explain ( >>> clf , >>> comparisons = [( 2 , 1 ), ( 3 , 1 )], >>> n_iterations = 10 , >>> downsample_background = True >>> ) Source code in dpks/quant_matrix.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 def explain ( self , clf , comparisons : list , n_iterations : int = 100 , downsample_background : bool = True , feature_column : str = \"Protein\" , ) -> QuantMatrix : \"\"\"Explain group differences using explainable machine learning and feature importance. Args: clf: Classifier object used for prediction. comparisons (list): List of tuples specifying the group comparisons. n_iterations (int, optional): Number of iterations for bootstrapping. Defaults to 100. downsample_background (bool, optional): Whether to downsample the background. Defaults to True. feature_column (str, optional): Name of the feature column. Defaults to 'Protein'. Returns: QuantMatrix: Matrix containing the results of the explanation. Examples: >>> import xgboost >>> >>> clf = xgboost.XGBClassifier( >>> max_depth=2, >>> reg_lambda=2, >>> objective=\"binary:logistic\", >>> seed=42 >>> ) >>> >>> quantified_data = quantified_data.explain( >>> clf, >>> comparisons=[(2, 1), (3, 1)], >>> n_iterations=10, >>> downsample_background=True >>> ) \"\"\" explain_results = [] for comparison in comparisons : X , y = self . to_ml ( feature_column = feature_column , comparison = comparison ) scaler = StandardScaler () X [:] = scaler . fit_transform ( X [:]) clf_ = Classifier ( clf ) interpreter = BootstrapInterpreter ( feature_names = X . columns , n_iterations = n_iterations , downsample_background = downsample_background , ) interpreter . fit ( X , y , clf_ ) explain_results . append (( comparison , interpreter )) importances_df = interpreter . importances [ [ \"feature\" , \"mean_shap\" , \"mean_rank\" ] ] . set_index ( \"feature\" ) importances_df = importances_df . rename ( columns = { \"mean_shap\" : f \"MeanSHAP { comparison [ 0 ] } - { comparison [ 1 ] } \" , \"mean_rank\" : f \"MeanRank { comparison [ 0 ] } - { comparison [ 1 ] } \" , } ) self . row_annotations = self . row_annotations . join ( importances_df , on = \"Protein\" ) self . explain_results = explain_results return self filter ( peptide_q_value = 0.01 , protein_q_value = 0.01 , remove_decoys = True , remove_contaminants = True , remove_non_proteotypic = True ) Filter the QuantMatrix. Parameters: peptide_q_value ( float , default: 0.01 ) \u2013 Peptide q-value threshold. Defaults to 0.01. protein_q_value ( float , default: 0.01 ) \u2013 Protein q-value threshold. Defaults to 0.01. remove_decoys ( bool , default: True ) \u2013 Whether to remove decoy entries. Defaults to True. remove_contaminants ( bool , default: True ) \u2013 Whether to remove contaminant entries. Defaults to True. remove_non_proteotypic ( bool , default: True ) \u2013 Whether to remove non-proteotypic entries. Defaults to True. Returns: QuantMatrix ( QuantMatrix ) \u2013 Filtered QuantMatrix object. Examples: >>> print ( quant_matrix . to_df () . shape ) (16679, 26) >>> print ( quant_matrix . filter ( peptide_q_value = 0.001 ) . to_df () . shape ) (15355, 26) Source code in dpks/quant_matrix.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def filter ( self , peptide_q_value : float = 0.01 , protein_q_value : float = 0.01 , remove_decoys : bool = True , remove_contaminants : bool = True , remove_non_proteotypic : bool = True , ) -> QuantMatrix : \"\"\"Filter the QuantMatrix. Args: peptide_q_value (float, optional): Peptide q-value threshold. Defaults to 0.01. protein_q_value (float, optional): Protein q-value threshold. Defaults to 0.01. remove_decoys (bool, optional): Whether to remove decoy entries. Defaults to True. remove_contaminants (bool, optional): Whether to remove contaminant entries. Defaults to True. remove_non_proteotypic (bool, optional): Whether to remove non-proteotypic entries. Defaults to True. Returns: QuantMatrix: Filtered QuantMatrix object. Examples: >>> print(quant_matrix.to_df().shape) (16679, 26) >>> print(quant_matrix.filter(peptide_q_value=0.001).to_df().shape) (15355, 26) \"\"\" filtered_data = self . quantitative_data if \"PeptideQValue\" in self . quantitative_data . obs : filtered_data = self . quantitative_data [ ( self . quantitative_data . obs [ \"PeptideQValue\" ] <= peptide_q_value ) ] . copy () if \"ProteinQValue\" in self . quantitative_data . obs : filtered_data = self . quantitative_data [ ( self . quantitative_data . obs [ \"ProteinQValue\" ] <= protein_q_value ) ] . copy () if remove_decoys : if \"Decoy\" in filtered_data . obs : filtered_data = filtered_data [ filtered_data . obs [ \"Decoy\" ] == 0 ] . copy () if remove_contaminants : filtered_data = filtered_data [ ~ filtered_data . obs [ \"Protein\" ] . str . contains ( \"contam\" ) ] . copy () filtered_data = filtered_data [ ~ filtered_data . obs [ \"Protein\" ] . str . contains ( \"cont_crap\" ) ] . copy () if remove_non_proteotypic : filtered_data = filtered_data [ ~ filtered_data . obs [ \"Protein\" ] . str . contains ( \";\" ) ] . copy () self . num_rows = len ( filtered_data ) quantitative_data = ( filtered_data . to_df ()[ list ( filtered_data . var [ \"sample\" ])] . copy () . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str )) ) row_obs = filtered_data . obs . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str ) ) self . quantitative_data = ad . AnnData ( quantitative_data , obs = row_obs , var = filtered_data . var , dtype = np . float64 , ) return self impute ( method , ** kwargs ) Impute missing values in the quantitative data. Parameters: method ( str ) \u2013 The imputation method to use. Options are \"uniform_percentile\" and \"uniform_range\" **kwargs ( int , default: {} ) \u2013 Additional keyword arguments specific to the imputation method. Returns: QuantMatrix ( QuantMatrix ) \u2013 The QuantMatrix object with missing values imputed. Raises: ValueError \u2013 If an unsupported imputation method is provided. Examples: >>> quant_matrix . impute ( method = \"uniform_percentile\" , percentile = 0.1 ) Source code in dpks/quant_matrix.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def impute ( self , method : str , ** kwargs : int ) -> QuantMatrix : \"\"\"Impute missing values in the quantitative data. Args: method (str): The imputation method to use. Options are \"uniform_percentile\" and \"uniform_range\" **kwargs (int): Additional keyword arguments specific to the imputation method. Returns: QuantMatrix: The QuantMatrix object with missing values imputed. Raises: ValueError: If an unsupported imputation method is provided. Examples: >>> quant_matrix.impute(method=\"uniform_percentile\", percentile=0.1) \"\"\" base_method : ImputerMethod = ImputerMethod () if method == \"uniform_percentile\" : percentile = float ( kwargs . get ( \"percentile\" , 0.1 )) base_method = UniformPercentileImputer ( percentile = percentile ) elif method == \"uniform_range\" : maxvalue = int ( kwargs . get ( \"maxvalue\" , 1 )) minvalue = int ( kwargs . get ( \"minvalue\" , 0 )) base_method = UniformRangeImputer ( maxvalue = maxvalue , minvalue = minvalue ) else : raise ValueError ( f \"Unsupported imputation method: { method } \" ) self . quantitative_data . X = base_method . fit_transform ( self . quantitative_data . X ) return self interpret ( classifier , scaler = None , shap_algorithm = 'auto' , scale = True , downsample_background = False ) Interpret the model's predictions using SHAP values. Parameters: classifier \u2013 The classifier model to interpret. scaler ( optional , default: None ) \u2013 The scaler object to use for data scaling. shap_algorithm ( str , default: 'auto' ) \u2013 The SHAP algorithm to use. Defaults to \"auto\". scale ( bool , default: True ) \u2013 Whether to scale the data before interpretation. Defaults to True. downsample_background ( bool , default: False ) \u2013 Whether to downsample background data. Defaults to False. Returns: QuantMatrix ( QuantMatrix ) \u2013 The QuantMatrix object with SHAP values added to observations. Examples: >>> quant_matrix . interpret ( classifier = clf , scaler = std_scaler ) Source code in dpks/quant_matrix.py 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 def interpret ( self , classifier , scaler : Any = None , shap_algorithm : str = \"auto\" , scale : bool = True , downsample_background = False , ) -> QuantMatrix : \"\"\"Interpret the model's predictions using SHAP values. Args: classifier: The classifier model to interpret. scaler (optional): The scaler object to use for data scaling. shap_algorithm (str): The SHAP algorithm to use. Defaults to \"auto\". scale (bool): Whether to scale the data before interpretation. Defaults to True. downsample_background (bool): Whether to downsample background data. Defaults to False. Returns: QuantMatrix: The QuantMatrix object with SHAP values added to observations. Examples: >>> quant_matrix.interpret(classifier=clf, scaler=std_scaler) \"\"\" X = format_data ( self ) y = encode_labels ( self . quantitative_data . var [ \"group\" ] . values ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) classifier = Classifier ( classifier = classifier , shap_algorithm = shap_algorithm ) if downsample_background : rus = RandomUnderSampler ( random_state = 0 ) X_resampled , y_resampled = rus . fit_resample ( X , y ) classifier . interpret ( X_resampled ) self . transformed_data = X_resampled self . y_resampled = y_resampled else : classifier . interpret ( X ) self . transformed_data = X self . classifier = classifier shap_values = classifier . feature_importances_ . tolist () self . quantitative_data . obs [ \"SHAP\" ] = shap_values self . shap = classifier . shap_values return self normalize ( method , log_transform = True , use_rt_sliding_window_filter = False , ** kwargs ) Normalize the QuantMatrix data. Parameters: method ( str ) \u2013 Normalization method. Options are 'tic', 'median', or 'mean'. log_transform ( bool , default: True ) \u2013 Whether to log-transform the data. Defaults to True. use_rt_sliding_window_filter ( bool , default: False ) \u2013 Whether to use a sliding window filter. Defaults to False. Can only use if a RetentionTime column was loaded in the QuantMatrix **kwargs ( Union [ int , bool , str ] , default: {} ) \u2013 Additional keyword arguments depending on the chosen method. Returns: QuantMatrix ( QuantMatrix ) \u2013 Normalized QuantMatrix object. Raises: ValueError \u2013 If the provided normalization method is not supported. Examples: >>> quant_matrix . normalize ( method = \"mean\" ) Source code in dpks/quant_matrix.py 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def normalize ( self , method : str , log_transform : bool = True , use_rt_sliding_window_filter : bool = False , ** kwargs : Union [ int , bool , str ], ) -> QuantMatrix : \"\"\"Normalize the QuantMatrix data. Args: method (str): Normalization method. Options are 'tic', 'median', or 'mean'. log_transform (bool, optional): Whether to log-transform the data. Defaults to True. use_rt_sliding_window_filter (bool, optional): Whether to use a sliding window filter. Defaults to False. Can only use if a RetentionTime column was loaded in the QuantMatrix **kwargs: Additional keyword arguments depending on the chosen method. Returns: QuantMatrix: Normalized QuantMatrix object. Raises: ValueError: If the provided normalization method is not supported. Examples: >>> quant_matrix.normalize(method=\"mean\") \"\"\" base_method : NormalizationMethod = NormalizationMethod () if method == \"tic\" : base_method = TicNormalization () elif method == \"median\" : base_method = MedianNormalization () elif method == \"mean\" : base_method = MeanNormalization () else : raise ValueError ( f \"Unsupported normalization method: { method } \" ) if use_rt_sliding_window_filter : minimum_data_points = int ( kwargs . get ( \"minimum_data_points\" , 100 )) stride = int ( kwargs . get ( \"stride\" , 1 )) use_overlapping_windows = bool ( kwargs . get ( \"use_overlapping_windows\" , True )) rt_unit = str ( kwargs . get ( \"rt_unit\" , \"minute\" )) rt_window_normalization = RTSlidingWindowNormalization ( base_method = base_method , minimum_data_points = minimum_data_points , stride = stride , use_overlapping_windows = use_overlapping_windows , rt_unit = rt_unit , ) self . quantitative_data . X = rt_window_normalization . fit_transform ( self ) else : self . quantitative_data . X = base_method . fit_transform ( self . quantitative_data . X ) if log_transform : self . quantitative_data . X = Log2Normalization () . fit_transform ( self . quantitative_data . X ) return self optimize ( classifier , param_search_method , param_grid , scaler = None , scale = True , threads = 1 , random_state = 42 , folds = 3 , verbose = False , ** kwargs ) Optimize hyperparameters of a classifier using different search methods. Parameters: classifier \u2013 The classifier object or class to optimize. param_search_method ( str ) \u2013 The parameter search method to use (\"genetic\" or \"random\"). param_grid ( dict ) \u2013 The parameter grid to search over. scaler ( Any , default: None ) \u2013 The scaler object to scale the data. Defaults to None. scale ( bool , default: True ) \u2013 Whether to scale the data. Defaults to True. threads ( int , default: 1 ) \u2013 The number of threads to use for optimization. Defaults to 1. random_state ( int , default: 42 ) \u2013 Random seed for reproducibility. Defaults to 42. folds ( int , default: 3 ) \u2013 The number of folds for cross-validation. Defaults to 3. verbose ( Union [ bool , int ] , default: False ) \u2013 Verbosity level. Defaults to False. **kwargs ( Union [ dict , int , str , bool ] , default: {} ) \u2013 Additional keyword arguments specific to each search method. Returns: ParamSearchResult ( ParamSearchResult ) \u2013 The result of the parameter search, including the best estimator and parameter populations. Examples: >>> param_grid = { 'max_depth' : [ 3 , 5 , 7 ], 'min_samples_split' : [ 2 , 5 , 10 ]} >>> result = quant_matrix . optimize ( classifier = DecisionTreeClassifier (), param_search_method = 'random' , param_grid = param_grid , verbose = True ) >>> result . best_estimator_ DecisionTreeClassifier(max_depth=5, min_samples_split=10) Source code in dpks/quant_matrix.py 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 def optimize ( self , classifier , param_search_method : str , param_grid : dict , scaler : Any = None , scale : bool = True , threads : int = 1 , random_state : int = 42 , folds : int = 3 , verbose : Union [ bool , int ] = False , ** kwargs : Union [ dict , int , str , bool ], ) -> ParamSearchResult : \"\"\"Optimize hyperparameters of a classifier using different search methods. Args: classifier: The classifier object or class to optimize. param_search_method (str): The parameter search method to use (\"genetic\" or \"random\"). param_grid (dict): The parameter grid to search over. scaler (Any): The scaler object to scale the data. Defaults to None. scale (bool): Whether to scale the data. Defaults to True. threads (int): The number of threads to use for optimization. Defaults to 1. random_state (int): Random seed for reproducibility. Defaults to 42. folds (int): The number of folds for cross-validation. Defaults to 3. verbose (Union[bool, int]): Verbosity level. Defaults to False. **kwargs: Additional keyword arguments specific to each search method. Returns: ParamSearchResult: The result of the parameter search, including the best estimator and parameter populations. Examples: >>> param_grid = {'max_depth': [3, 5, 7], 'min_samples_split': [2, 5, 10]} >>> result = quant_matrix.optimize(classifier=DecisionTreeClassifier(), param_search_method='random', param_grid=param_grid, verbose=True) >>> result.best_estimator_ DecisionTreeClassifier(max_depth=5, min_samples_split=10) \"\"\" X = format_data ( self ) y = encode_labels ( self . quantitative_data . var [ \"group\" ] . values ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) result = None if param_search_method == \"genetic\" : gas = GeneticAlgorithmSearch ( classifier , param_grid = param_grid , threads = threads , folds = folds , n_survive = kwargs . get ( \"n_survive\" , 5 ), pop_size = kwargs . get ( \"pop_size\" , 10 ), n_generations = kwargs . get ( \"n_generations\" , 20 ), verbose = verbose , random_state = kwargs . get ( \"random_state\" , None ), shuffle = kwargs . get ( \"shuffle\" , False ), ) parameter_populations = gas . fit ( X , y ) result = ParamSearchResult ( classifier = gas . best_estimator_ , result = parameter_populations , ) elif param_search_method == \"random\" : randomized_search = RandomizedSearch ( classifier , param_grid = param_grid , folds = folds , random_state = random_state , n_iter = kwargs . get ( \"n_iter\" , 30 ), n_jobs = threads , scoring = kwargs . get ( \"scoring\" , \"accuracy\" ), verbose = verbose , ) result = randomized_search . fit ( X , y ) return result plot ( plot_type , save = False , fig = None , ax = None , ** kwargs ) Generate plots based on specified plot type. Parameters: plot_type ( str ) \u2013 The type of plot to generate. Possible values are: - \"shap_summary\": SHAP summary plot. - \"rfe_pca\": Recursive Feature Elimination (RFE) with Principal Component Analysis (PCA) plot. save ( bool , default: False ) \u2013 Whether to save the plot. Defaults to False. fig ( Figure , default: None ) \u2013 The matplotlib figure object. Defaults to None. ax ( Union [ list , Axes ] , default: None ) \u2013 The list of matplotlib axes objects or a single axes object. Defaults to None. **kwargs ( Union [ ndarray , int , list , str ] , default: {} ) \u2013 Additional keyword arguments specific to each plot type. Returns: tuple [ Figure , Axes ] \u2013 tuple[matplotlib.figure.Figure, matplotlib.axes.Axes]: The matplotlib figure and axes objects. Raises: ValueError \u2013 If an unsupported plot type is provided. Examples: >>> fig , ax = quant_matrix . plot ( plot_type = 'shap_summary' , save = True , n_display = 10 ) Source code in dpks/quant_matrix.py 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 def plot ( self , plot_type : str , save : bool = False , fig : matplotlib . figure . Figure = None , ax : Union [ list , matplotlib . axes . Axes ] = None , ** kwargs : Union [ np . ndarray , int , list , str , ], ) -> tuple [ matplotlib . figure . Figure , matplotlib . axes . Axes ]: \"\"\"Generate plots based on specified plot type. Args: plot_type (str): The type of plot to generate. Possible values are: - \"shap_summary\": SHAP summary plot. - \"rfe_pca\": Recursive Feature Elimination (RFE) with Principal Component Analysis (PCA) plot. save (bool): Whether to save the plot. Defaults to False. fig (matplotlib.figure.Figure): The matplotlib figure object. Defaults to None. ax (Union[list, matplotlib.axes.Axes]): The list of matplotlib axes objects or a single axes object. Defaults to None. **kwargs: Additional keyword arguments specific to each plot type. Returns: tuple[matplotlib.figure.Figure, matplotlib.axes.Axes]: The matplotlib figure and axes objects. Raises: ValueError: If an unsupported plot type is provided. Examples: >>> fig, ax = quant_matrix.plot(plot_type='shap_summary', save=True, n_display=10) \"\"\" if plot_type == \"shap_summary\" : try : getattr ( self , \"shap\" ) except AttributeError : print ( \"SHAP values have not been generated\" ) cmap = kwargs . get ( \"cmap\" , [ \"#ff4800\" , \"#ff4040\" , \"#a836ff\" , \"#405cff\" , \"#05c9fa\" , ], ) order_by = kwargs . get ( \"order_by\" , \"shap\" ) fig , ax = SHAPPlot ( fig = fig , ax = ax , shap_values = self . shap , X = self . transformed_data , qm = self , cmap = cmap , n_display = kwargs . get ( \"n_display\" , 5 ), jitter = kwargs . get ( \"jitter\" , 0.1 ), alpha = kwargs . get ( \"alpha\" , 0.75 ), n_bins = kwargs . get ( \"n_bins\" , 100 ), feature_column = kwargs . get ( \"feature_column\" , \"Protein\" ), order_by = order_by , ) . plot () if plot_type == \"rfe_pca\" : cmap = kwargs . get ( \"cmap\" , \"coolwarm\" ) cutoffs = list ( kwargs . get ( \"cutoffs\" , [ 100 , 50 , 10 ])) fig , ax = RFEPCA ( fig = fig , axs = ax , qm = self , cutoffs = cutoffs , cmap = cmap ) . plot () if save : filepath = str ( kwargs . get ( \"filepath\" , f \" { plot_type } .png\" )) dpi = int ( kwargs . get ( \"dpi\" , 300 )) matplotlib . pyplot . savefig ( filepath , dpi = dpi ) return fig , ax predict ( classifier , scaler = None , scale = True ) Predict labels using a classifier. Parameters: classifier \u2013 The classifier model to use for prediction. scaler ( optional , default: None ) \u2013 The scaler object to use for data scaling. scale ( bool , default: True ) \u2013 Whether to scale the data before prediction. Defaults to True. Returns: QuantMatrix ( QuantMatrix ) \u2013 The QuantMatrix object with predicted labels. Examples: >>> quant_matrix . predict ( classifier = clf , scaler = std_scaler ) Source code in dpks/quant_matrix.py 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 def predict ( self , classifier , scaler : Any = None , scale : bool = True , ) -> QuantMatrix : \"\"\"Predict labels using a classifier. Args: classifier: The classifier model to use for prediction. scaler (optional): The scaler object to use for data scaling. scale (bool): Whether to scale the data before prediction. Defaults to True. Returns: QuantMatrix: The QuantMatrix object with predicted labels. Examples: >>> quant_matrix.predict(classifier=clf, scaler=std_scaler) \"\"\" X = format_data ( self ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) classifier = Classifier ( classifier = classifier ) self . sample_annotations [ \"Prediction\" ] = classifier . predict ( X ) return self quantify ( method , ** kwargs ) Calculate protein quantities. Parameters: method ( str ) \u2013 Quantification method. Options are 'top_n' or 'maxlfq'. **kwargs ( Union [ int , str ] , default: {} ) \u2013 Additional keyword arguments depending on the chosen method. Returns: QuantMatrix ( QuantMatrix ) \u2013 Quantified protein matrix. Raises: ValueError \u2013 If the provided quantification method is not supported. Examples: >>> quant_matrix . quantify ( method = \"top_n\" , top_n = 1 ) Source code in dpks/quant_matrix.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def quantify ( self , method : str , ** kwargs : Union [ int , str ], ) -> QuantMatrix : \"\"\"Calculate protein quantities. Args: method (str): Quantification method. Options are 'top_n' or 'maxlfq'. **kwargs: Additional keyword arguments depending on the chosen method. Returns: QuantMatrix: Quantified protein matrix. Raises: ValueError: If the provided quantification method is not supported. Examples: >>> quant_matrix.quantify(method=\"top_n\", top_n=1) \"\"\" if method == \"top_n\" : level = str ( kwargs . get ( \"level\" , \"protein\" )) top_n = int ( kwargs . get ( \"top_n\" , 1 )) summarization_method = str ( kwargs . get ( \"summarization_method\" , \"sum\" )) quantifications = TopN ( top_n = top_n , level = level , summarization_method = summarization_method ) . quantify ( self ) design_matrix = self . quantitative_data . var protein_quantifications = QuantMatrix ( quantifications , design_matrix_file = design_matrix ) elif method == \"maxlfq\" : level = str ( kwargs . get ( \"level\" , \"protein\" )) threads = int ( kwargs . get ( \"threads\" , 1 )) minimum_subgroups = int ( kwargs . get ( \"minimum_subgroups\" , 1 )) top_n = int ( kwargs . get ( \"top_n\" , 0 )) quantifications = MaxLFQ ( level = level , threads = threads , minimum_subgroups = minimum_subgroups , top_n = top_n , ) . quantify ( self ) design_matrix = self . quantitative_data . var protein_quantifications = QuantMatrix ( quantifications , design_matrix_file = design_matrix ) else : raise ValueError ( f \"Unsupported quantification method: { method } \" ) return protein_quantifications scale ( method ) Scale the QuantMatrix data at the feature level (i.e Precursor or Protein). Parameters: method ( str ) \u2013 Scaling method. Options are 'zscore', 'minmax', or 'absmax'. Returns: QuantMatrix ( QuantMatrix ) \u2013 Scaled QuantMatrix object. Raises: ValueError \u2013 If the provided scaling method is not supported. Source code in dpks/quant_matrix.py 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 def scale ( self , method : str , ) -> QuantMatrix : \"\"\"Scale the QuantMatrix data at the feature level (i.e Precursor or Protein). Args: method (str): Scaling method. Options are 'zscore', 'minmax', or 'absmax'. Returns: QuantMatrix: Scaled QuantMatrix object. Raises: ValueError: If the provided scaling method is not supported. \"\"\" base_method : ScalingMethod = ScalingMethod () if method == \"zscore\" : base_method = ZScoreScaling () elif method == \"minmax\" : base_method = MinMaxScaling () elif method == \"absmax\" : base_method = AbsMaxScaling () else : raise ValueError ( f \"Unsupported scaling method: { method } \" ) self . quantitative_data . X = base_method . fit_transform ( self . quantitative_data . X ) return self to_df () Convert the QuantMatrix object to a pandas DataFrame. Returns: DataFrame \u2013 pd.DataFrame: DataFrame representation of the QuantMatrix. Examples: >>> quant_matrix . to_df () Source code in dpks/quant_matrix.py 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 def to_df ( self ) -> pd . DataFrame : \"\"\"Convert the QuantMatrix object to a pandas DataFrame. Returns: pd.DataFrame: DataFrame representation of the QuantMatrix. Examples: >>> quant_matrix.to_df() \"\"\" quant_data = self . quantitative_data [ self . row_annotations . index , :] . to_df () merged = pd . concat ([ self . row_annotations , quant_data ], axis = 1 ) return merged to_ml ( feature_column = 'Protein' , label_column = 'group' , comparison = ( 1 , 2 )) Converts the QuantMatrix object to features and labels for machine learning. Parameters: feature_column ( str , default: 'Protein' ) \u2013 The column to use as features. Defaults to \"Protein\". label_column ( str , default: 'group' ) \u2013 The column to use as labels. Defaults to \"group\". comparison ( tuple , default: (1, 2) ) \u2013 The comparison groups. Defaults to (1, 2). Returns: tuple [ Any , Any ] \u2013 tuple[Any, Any]: A tuple containing features and labels. Examples: >>> features , labels = quant_matrix . to_ml () Source code in dpks/quant_matrix.py 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 def to_ml ( self , feature_column : str = \"Protein\" , label_column : str = \"group\" , comparison : tuple = ( 1 , 2 ), ) -> tuple [ Any , Any ]: \"\"\"Converts the QuantMatrix object to features and labels for machine learning. Args: feature_column (str, optional): The column to use as features. Defaults to \"Protein\". label_column (str, optional): The column to use as labels. Defaults to \"group\". comparison (tuple, optional): The comparison groups. Defaults to (1, 2). Returns: tuple[Any, Any]: A tuple containing features and labels. Examples: >>> features, labels = quant_matrix.to_ml() \"\"\" qm_df = self . to_df () samples = self . sample_annotations [ self . sample_annotations [ \"group\" ] . isin ( comparison ) ][ \"sample\" ] . to_list () transposed_features = qm_df . set_index ( feature_column )[ samples ] . T sample_annotations = self . sample_annotations . copy () sample_annotations_subset = sample_annotations [ sample_annotations [ label_column ] . isin ( comparison ) ] . copy () encoder = LabelEncoder () sample_annotations_subset [ \"label\" ] = encoder . fit_transform ( sample_annotations_subset [ label_column ] ) combined = transposed_features . join ( sample_annotations_subset [[ \"sample\" , \"label\" ]] . set_index ( \"sample\" ), how = \"left\" , ) return combined . loc [:, combined . columns != \"label\" ], combined [[ \"label\" ]] train ( classifier , scaler = None , scale = True , validate = True , scoring = 'accuracy' , num_folds = 3 , random_state = 42 , shuffle = False ) Train a classifier on the quantitative data. Parameters: classifier \u2013 The classifier object or class to use for training. scaler ( Any , default: None ) \u2013 The scaler object to scale the data. Defaults to None. scale ( bool , default: True ) \u2013 Whether to scale the data. Defaults to True. validate ( bool , default: True ) \u2013 Whether to perform cross-validation. Defaults to True. scoring ( str , default: 'accuracy' ) \u2013 The scoring metric for cross-validation. Defaults to \"accuracy\". num_folds ( int , default: 3 ) \u2013 The number of folds for cross-validation. Defaults to 3. random_state ( int , default: 42 ) \u2013 Random seed for reproducibility. Defaults to 42. shuffle ( bool , default: False ) \u2013 Whether to shuffle the data before splitting in cross-validation. Defaults to False. Returns: TrainResult ( TrainResult ) \u2013 The result of the training process, including the trained classifier, scaler, and validation scores. Examples: >>> result = quant_matrix . train ( classifier = RandomForestClassifier (), validate = True ) Source code in dpks/quant_matrix.py 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 def train ( self , classifier , scaler : Any = None , scale : bool = True , validate : bool = True , scoring : str = \"accuracy\" , num_folds : int = 3 , random_state : int = 42 , shuffle : bool = False , ) -> TrainResult : \"\"\"Train a classifier on the quantitative data. Args: classifier: The classifier object or class to use for training. scaler (Any): The scaler object to scale the data. Defaults to None. scale (bool): Whether to scale the data. Defaults to True. validate (bool): Whether to perform cross-validation. Defaults to True. scoring (str): The scoring metric for cross-validation. Defaults to \"accuracy\". num_folds (int): The number of folds for cross-validation. Defaults to 3. random_state (int): Random seed for reproducibility. Defaults to 42. shuffle (bool): Whether to shuffle the data before splitting in cross-validation. Defaults to False. Returns: TrainResult: The result of the training process, including the trained classifier, scaler, and validation scores. Examples: >>> result = quant_matrix.train(classifier=RandomForestClassifier(), validate=True) \"\"\" X = format_data ( self ) y = encode_labels ( self . quantitative_data . var [ \"group\" ] . values ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) classifier = Classifier ( classifier = classifier ) validation_result = np . array ([]) if validate : cv = StratifiedKFold ( num_folds , shuffle = shuffle , random_state = random_state ) validation_result = cross_val_score ( classifier , X , y , scoring = scoring , cv = cv ) classifier . fit ( X , y ) return TrainResult ( classifier , scaler , validation_result ) write ( file_path ) Write the QuantMatrix to a tab-separated file. Parameters: file_path ( str ) \u2013 The path where the file will be saved. Returns: None \u2013 None Examples: >>> filename = \"protein.tsv\" >>> quant_matrix . write ( filename ) Source code in dpks/quant_matrix.py 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 def write ( self , file_path : str ) -> None : \"\"\"Write the QuantMatrix to a tab-separated file. Args: file_path (str): The path where the file will be saved. Returns: None Examples: >>> filename = \"protein.tsv\" >>> quant_matrix.write(filename) \"\"\" self . to_df () . to_csv ( file_path , sep = \" \\t \" , index = False )","title":"QuantMatrix"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.__init__","text":"Initialize the QuantMatrix instance. Parameters: quantification_file ( Union [ str , DataFrame ] ) \u2013 Path to the quantification file or DataFrame. design_matrix_file ( Union [ str , DataFrame ] ) \u2013 Path to the design matrix file or DataFrame. annotation_fasta_file ( str , default: None ) \u2013 Path to the annotation FASTA file. Defaults to None. quant_type ( str , default: 'gps' ) \u2013 Type of quantification. Defaults to \"gps\". diann_qvalue ( float , default: 0.01 ) \u2013 DIANN q-value. Defaults to 0.01. Examples: >>> quant_matrix = QuantMatrix ( \"quantification.tsv\" , \"design_matrix.csv\" , annotation_fasta_file = \"annotation.fasta\" ) Source code in dpks/quant_matrix.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def __init__ ( self , quantification_file : Union [ str , pd . DataFrame ], design_matrix_file : Union [ str , pd . DataFrame ], annotation_fasta_file : str = None , quant_type : str = \"gps\" , diann_qvalue : float = 0.01 , ) -> None : \"\"\"Initialize the QuantMatrix instance. Args: quantification_file (Union[str, pd.DataFrame]): Path to the quantification file or DataFrame. design_matrix_file (Union[str, pd.DataFrame]): Path to the design matrix file or DataFrame. annotation_fasta_file (str, optional): Path to the annotation FASTA file. Defaults to None. quant_type (str, optional): Type of quantification. Defaults to \"gps\". diann_qvalue (float, optional): DIANN q-value. Defaults to 0.01. Examples: >>> quant_matrix = QuantMatrix(\"quantification.tsv\", \"design_matrix.csv\", annotation_fasta_file=\"annotation.fasta\") \"\"\" self . annotated = False self . explain_results = None if isinstance ( design_matrix_file , str ): design_matrix_file = pd . read_csv ( design_matrix_file , sep = \" \\t \" ) design_matrix_file . columns = map ( str . lower , design_matrix_file . columns ) if isinstance ( quantification_file , str ): if quant_type == \"gps\" : quantification_file = pd . read_csv ( quantification_file , sep = \" \\t \" ) elif quant_type == \"diann\" : quantification_file = parse_diann ( quantification_file , diann_qvalue ) else : if quant_type == \"diann\" : quantification_file = parse_diann ( quantification_file , diann_qvalue ) self . num_samples = len ( design_matrix_file ) self . num_rows = len ( quantification_file ) rt_column = \"\" if \"RT\" in quantification_file : rt_column = \"RT\" elif \"RetentionTime\" in quantification_file : rt_column = \"RetentionTime\" if rt_column : quantification_file = quantification_file . sort_values ( rt_column ) quantification_file = quantification_file . reset_index ( drop = True ) quantitative_data = ( quantification_file [ list ( design_matrix_file [ \"sample\" ])] . copy () . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str )) ) row_obs = quantification_file . drop ( list ( design_matrix_file [ \"sample\" ]), axis = 1 ) . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str )) if annotation_fasta_file is not None : row_obs [ \"ProteinLabel\" ] = get_protein_labels ( row_obs [ \"Protein\" ], annotation_fasta_file ) self . quantitative_data = ad . AnnData ( quantitative_data , obs = row_obs , var = design_matrix_file . copy () . set_index ( design_matrix_file [ \"sample\" ]), dtype = np . float64 , )","title":"__init__"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.annotate","text":"Annotate proteins with gene names. Returns: QuantMatrix \u2013 The annotated QuantMatrix object. Examples: >>> quant_matrix . annotate () Source code in dpks/quant_matrix.py 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 def annotate ( self ): \"\"\"Annotate proteins with gene names. Returns: QuantMatrix: The annotated QuantMatrix object. Examples: >>> quant_matrix.annotate() \"\"\" request = IdMappingClient . submit ( source = \"UniProtKB_AC-ID\" , dest = \"Gene_Name\" , ids = self . proteins ) while True : status = request . get_status () if status in { \"FINISHED\" , \"ERROR\" }: break else : time . sleep ( 1 ) translation_result = list ( request . each_result ()) id_mapping = dict () for id_result in translation_result : mapping = id_mapping . get ( id_result [ \"from\" ], []) mapping . append ( id_result [ \"to\" ]) id_mapping [ id_result [ \"from\" ]] = mapping final_mapping = dict () for key , value in id_mapping . items (): value = value [ 0 ] final_mapping [ key ] = value mapping_df = pd . DataFrame ( { \"Protein\" : final_mapping . keys (), \"Gene\" : final_mapping . values ()} ) self . row_annotations = self . row_annotations . join ( mapping_df . set_index ( \"Protein\" ), on = \"Protein\" , how = \"left\" ) self . row_annotations [ \"Gene\" ] = self . row_annotations [ \"Gene\" ] . fillna ( self . row_annotations [ \"Protein\" ] ) self . annotated = True return self","title":"annotate"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.compare","text":"Compare groups by differential testing. Parameters: method ( str ) \u2013 Statistical comparison method. Options are 'ttest', 'linregress', 'anova', 'ttest_paired'. comparisons ( list ) \u2013 List of tuples specifying the group comparisons. min_samples_per_group ( int , default: 2 ) \u2013 Minimum number of samples per group. Defaults to 2. level ( str , default: 'protein' ) \u2013 Level of comparison. Defaults to 'protein'. multiple_testing_correction_method ( str , default: 'fdr_tsbh' ) \u2013 Method for multiple testing correction. Defaults to 'fdr_tsbh'. Returns: QuantMatrix ( QuantMatrix ) \u2013 Matrix containing the results of the differential testing. Raises: ValueError \u2013 If the provided statistical comparison method is not supported. Examples: >>> quantified_data = quantified_data . compare ( >>> method = \"linregress\" , >>> min_samples_per_group = 2 , >>> comparisons = [( 2 , 1 ), ( 3 , 1 )] >>> ) Source code in dpks/quant_matrix.py 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 def compare ( self , method : str , comparisons : list , min_samples_per_group : int = 2 , level : str = \"protein\" , multiple_testing_correction_method : str = \"fdr_tsbh\" , ) -> QuantMatrix : \"\"\"Compare groups by differential testing. Args: method (str): Statistical comparison method. Options are 'ttest', 'linregress', 'anova', 'ttest_paired'. comparisons (list): List of tuples specifying the group comparisons. min_samples_per_group (int, optional): Minimum number of samples per group. Defaults to 2. level (str, optional): Level of comparison. Defaults to 'protein'. multiple_testing_correction_method (str, optional): Method for multiple testing correction. Defaults to 'fdr_tsbh'. Returns: QuantMatrix: Matrix containing the results of the differential testing. Raises: ValueError: If the provided statistical comparison method is not supported. Examples: >>> quantified_data = quantified_data.compare( >>> method=\"linregress\", >>> min_samples_per_group=2, >>> comparisons=[(2, 1), (3, 1)] >>> ) \"\"\" if not method in { 'ttest' , 'linregress' , 'anova' , 'ttest_paired' }: raise ValueError ( f \"Unsupported statistical comparison method: { method } \" ) differential_test = DifferentialTest ( method , comparisons , min_samples_per_group , level , multiple_testing_correction_method , ) compared_data = differential_test . test ( self ) self . row_annotations = compared_data . row_annotations . copy () return self","title":"compare"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.detect","text":"Not implemented Detect outliers in the samples Source code in dpks/quant_matrix.py 1106 1107 1108 1109 1110 1111 1112 def detect ( self ) -> None : \"\"\"Not implemented Detect outliers in the samples \"\"\" pass","title":"detect"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.enrich","text":"Perform gene set enrichment analysis. Parameters: method ( str , default: 'overreptest' ) \u2013 Enrichment method to use. Options are \"enrichr_overreptest\" and \"overreptest\". Defaults to \"overreptest\". libraries ( Optional [ List [ str ]] , default: None ) \u2013 List of gene set libraries. Defaults to None. organism ( str , default: 'human' ) \u2013 Organism for the analysis. Defaults to \"human\". background ( Optional [ Union [ List [ str ], str ]] , default: None ) \u2013 Background gene set. Defaults to None. filter_pvalue ( bool , default: False ) \u2013 Whether to filter by p-value. Defaults to False. pvalue_cutoff ( float , default: 0.1 ) \u2013 P-value cutoff for filtering. Defaults to 0.1. pvalue_column ( str , default: 'CorrectedPValue2-1' ) \u2013 Column name for p-values. Defaults to \"CorrectedPValue2-1\". filter_shap ( bool , default: False ) \u2013 Whether to filter by SHAP value. Defaults to False. shap_cutoff ( float , default: 0.0 ) \u2013 SHAP value cutoff for filtering. Defaults to 0.0. shap_column ( str , default: 'MeanSHAP2-1' ) \u2013 Column name for SHAP values. Defaults to \"MeanSHAP2-1\". subset_library ( bool , default: False ) \u2013 Whether to subset the library. Defaults to False. Returns: Any \u2013 Enrichment result. Raises: ValueError \u2013 If the method is not supported. Examples: >>> enr = quantified_data . enrich ( >>> method = \"enrichr_overreptest\" , >>> filter_pvalue = True , >>> pvalue_column = \"CorrectedPValue2-1\" , >>> pvalue_cutoff = 0.1 >>> ) Source code in dpks/quant_matrix.py 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 def enrich ( self , method : str = \"overreptest\" , libraries : Optional [ list [ str ]] = None , organism : str = \"human\" , background : Optional [ Union [ list [ str ], str ]] = None , filter_pvalue : bool = False , pvalue_cutoff : float = 0.1 , pvalue_column : str = \"CorrectedPValue2-1\" , filter_shap : bool = False , shap_cutoff : float = 0.0 , shap_column : str = \"MeanSHAP2-1\" , subset_library : bool = False , ): \"\"\"Perform gene set enrichment analysis. Args: method (str, optional): Enrichment method to use. Options are \"enrichr_overreptest\" and \"overreptest\". Defaults to \"overreptest\". libraries (Optional[List[str]], optional): List of gene set libraries. Defaults to None. organism (str, optional): Organism for the analysis. Defaults to \"human\". background (Optional[Union[List[str], str]], optional): Background gene set. Defaults to None. filter_pvalue (bool, optional): Whether to filter by p-value. Defaults to False. pvalue_cutoff (float, optional): P-value cutoff for filtering. Defaults to 0.1. pvalue_column (str, optional): Column name for p-values. Defaults to \"CorrectedPValue2-1\". filter_shap (bool, optional): Whether to filter by SHAP value. Defaults to False. shap_cutoff (float, optional): SHAP value cutoff for filtering. Defaults to 0.0. shap_column (str, optional): Column name for SHAP values. Defaults to \"MeanSHAP2-1\". subset_library (bool, optional): Whether to subset the library. Defaults to False. Returns: Any: Enrichment result. Raises: ValueError: If the method is not supported. Examples: >>> enr = quantified_data.enrich( >>> method=\"enrichr_overreptest\", >>> filter_pvalue=True, >>> pvalue_column=\"CorrectedPValue2-1\", >>> pvalue_cutoff=0.1 >>> ) \"\"\" if not self . annotated : self . annotate () if not libraries : libraries = [ \"GO_Biological_Process_2023\" ] gene_df = pd . DataFrame () if filter_pvalue : gene_df = self . row_annotations [ self . row_annotations [ pvalue_column ] < pvalue_cutoff ] if filter_shap : gene_df = self . row_annotations [ self . row_annotations [ shap_column ] > shap_cutoff ] genes = gene_df [ \"Gene\" ] . to_list () if subset_library : temp_libraries = [] for library in libraries : go_bp = gp . get_library ( name = library , organism = organism ) gene_set = set ( gene_df [ \"Gene\" ] . to_list ()) bio_process_subset = dict () for key , value in go_bp . items (): for gene in value : if gene in gene_set : bio_process_subset [ key ] = value temp_libraries . append ( bio_process_subset ) libraries = temp_libraries enr = None if method == \"overreptest\" : if background : enr = gp . enrich ( gene_list = genes , gene_sets = libraries , background = background , ) else : enr = gp . enrich ( gene_list = genes , gene_sets = libraries ) elif method == \"enrichr_overreptest\" : if background : enr = gp . enrichr ( gene_list = genes , gene_sets = libraries , organism = organism , background = background , ) else : enr = gp . enrichr ( gene_list = genes , gene_sets = libraries , organism = organism , ) else : raise ValueError ( f \"Unsupported pathway enrichment method: { method } \" ) return enr","title":"enrich"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.explain","text":"Explain group differences using explainable machine learning and feature importance. Parameters: clf \u2013 Classifier object used for prediction. comparisons ( list ) \u2013 List of tuples specifying the group comparisons. n_iterations ( int , default: 100 ) \u2013 Number of iterations for bootstrapping. Defaults to 100. downsample_background ( bool , default: True ) \u2013 Whether to downsample the background. Defaults to True. feature_column ( str , default: 'Protein' ) \u2013 Name of the feature column. Defaults to 'Protein'. Returns: QuantMatrix ( QuantMatrix ) \u2013 Matrix containing the results of the explanation. Examples: >>> import xgboost >>> >>> clf = xgboost . XGBClassifier ( >>> max_depth = 2 , >>> reg_lambda = 2 , >>> objective = \"binary:logistic\" , >>> seed = 42 >>> ) >>> >>> quantified_data = quantified_data . explain ( >>> clf , >>> comparisons = [( 2 , 1 ), ( 3 , 1 )], >>> n_iterations = 10 , >>> downsample_background = True >>> ) Source code in dpks/quant_matrix.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 def explain ( self , clf , comparisons : list , n_iterations : int = 100 , downsample_background : bool = True , feature_column : str = \"Protein\" , ) -> QuantMatrix : \"\"\"Explain group differences using explainable machine learning and feature importance. Args: clf: Classifier object used for prediction. comparisons (list): List of tuples specifying the group comparisons. n_iterations (int, optional): Number of iterations for bootstrapping. Defaults to 100. downsample_background (bool, optional): Whether to downsample the background. Defaults to True. feature_column (str, optional): Name of the feature column. Defaults to 'Protein'. Returns: QuantMatrix: Matrix containing the results of the explanation. Examples: >>> import xgboost >>> >>> clf = xgboost.XGBClassifier( >>> max_depth=2, >>> reg_lambda=2, >>> objective=\"binary:logistic\", >>> seed=42 >>> ) >>> >>> quantified_data = quantified_data.explain( >>> clf, >>> comparisons=[(2, 1), (3, 1)], >>> n_iterations=10, >>> downsample_background=True >>> ) \"\"\" explain_results = [] for comparison in comparisons : X , y = self . to_ml ( feature_column = feature_column , comparison = comparison ) scaler = StandardScaler () X [:] = scaler . fit_transform ( X [:]) clf_ = Classifier ( clf ) interpreter = BootstrapInterpreter ( feature_names = X . columns , n_iterations = n_iterations , downsample_background = downsample_background , ) interpreter . fit ( X , y , clf_ ) explain_results . append (( comparison , interpreter )) importances_df = interpreter . importances [ [ \"feature\" , \"mean_shap\" , \"mean_rank\" ] ] . set_index ( \"feature\" ) importances_df = importances_df . rename ( columns = { \"mean_shap\" : f \"MeanSHAP { comparison [ 0 ] } - { comparison [ 1 ] } \" , \"mean_rank\" : f \"MeanRank { comparison [ 0 ] } - { comparison [ 1 ] } \" , } ) self . row_annotations = self . row_annotations . join ( importances_df , on = \"Protein\" ) self . explain_results = explain_results return self","title":"explain"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.filter","text":"Filter the QuantMatrix. Parameters: peptide_q_value ( float , default: 0.01 ) \u2013 Peptide q-value threshold. Defaults to 0.01. protein_q_value ( float , default: 0.01 ) \u2013 Protein q-value threshold. Defaults to 0.01. remove_decoys ( bool , default: True ) \u2013 Whether to remove decoy entries. Defaults to True. remove_contaminants ( bool , default: True ) \u2013 Whether to remove contaminant entries. Defaults to True. remove_non_proteotypic ( bool , default: True ) \u2013 Whether to remove non-proteotypic entries. Defaults to True. Returns: QuantMatrix ( QuantMatrix ) \u2013 Filtered QuantMatrix object. Examples: >>> print ( quant_matrix . to_df () . shape ) (16679, 26) >>> print ( quant_matrix . filter ( peptide_q_value = 0.001 ) . to_df () . shape ) (15355, 26) Source code in dpks/quant_matrix.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def filter ( self , peptide_q_value : float = 0.01 , protein_q_value : float = 0.01 , remove_decoys : bool = True , remove_contaminants : bool = True , remove_non_proteotypic : bool = True , ) -> QuantMatrix : \"\"\"Filter the QuantMatrix. Args: peptide_q_value (float, optional): Peptide q-value threshold. Defaults to 0.01. protein_q_value (float, optional): Protein q-value threshold. Defaults to 0.01. remove_decoys (bool, optional): Whether to remove decoy entries. Defaults to True. remove_contaminants (bool, optional): Whether to remove contaminant entries. Defaults to True. remove_non_proteotypic (bool, optional): Whether to remove non-proteotypic entries. Defaults to True. Returns: QuantMatrix: Filtered QuantMatrix object. Examples: >>> print(quant_matrix.to_df().shape) (16679, 26) >>> print(quant_matrix.filter(peptide_q_value=0.001).to_df().shape) (15355, 26) \"\"\" filtered_data = self . quantitative_data if \"PeptideQValue\" in self . quantitative_data . obs : filtered_data = self . quantitative_data [ ( self . quantitative_data . obs [ \"PeptideQValue\" ] <= peptide_q_value ) ] . copy () if \"ProteinQValue\" in self . quantitative_data . obs : filtered_data = self . quantitative_data [ ( self . quantitative_data . obs [ \"ProteinQValue\" ] <= protein_q_value ) ] . copy () if remove_decoys : if \"Decoy\" in filtered_data . obs : filtered_data = filtered_data [ filtered_data . obs [ \"Decoy\" ] == 0 ] . copy () if remove_contaminants : filtered_data = filtered_data [ ~ filtered_data . obs [ \"Protein\" ] . str . contains ( \"contam\" ) ] . copy () filtered_data = filtered_data [ ~ filtered_data . obs [ \"Protein\" ] . str . contains ( \"cont_crap\" ) ] . copy () if remove_non_proteotypic : filtered_data = filtered_data [ ~ filtered_data . obs [ \"Protein\" ] . str . contains ( \";\" ) ] . copy () self . num_rows = len ( filtered_data ) quantitative_data = ( filtered_data . to_df ()[ list ( filtered_data . var [ \"sample\" ])] . copy () . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str )) ) row_obs = filtered_data . obs . set_index ( np . arange ( self . num_rows , dtype = int ) . astype ( str ) ) self . quantitative_data = ad . AnnData ( quantitative_data , obs = row_obs , var = filtered_data . var , dtype = np . float64 , ) return self","title":"filter"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.impute","text":"Impute missing values in the quantitative data. Parameters: method ( str ) \u2013 The imputation method to use. Options are \"uniform_percentile\" and \"uniform_range\" **kwargs ( int , default: {} ) \u2013 Additional keyword arguments specific to the imputation method. Returns: QuantMatrix ( QuantMatrix ) \u2013 The QuantMatrix object with missing values imputed. Raises: ValueError \u2013 If an unsupported imputation method is provided. Examples: >>> quant_matrix . impute ( method = \"uniform_percentile\" , percentile = 0.1 ) Source code in dpks/quant_matrix.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def impute ( self , method : str , ** kwargs : int ) -> QuantMatrix : \"\"\"Impute missing values in the quantitative data. Args: method (str): The imputation method to use. Options are \"uniform_percentile\" and \"uniform_range\" **kwargs (int): Additional keyword arguments specific to the imputation method. Returns: QuantMatrix: The QuantMatrix object with missing values imputed. Raises: ValueError: If an unsupported imputation method is provided. Examples: >>> quant_matrix.impute(method=\"uniform_percentile\", percentile=0.1) \"\"\" base_method : ImputerMethod = ImputerMethod () if method == \"uniform_percentile\" : percentile = float ( kwargs . get ( \"percentile\" , 0.1 )) base_method = UniformPercentileImputer ( percentile = percentile ) elif method == \"uniform_range\" : maxvalue = int ( kwargs . get ( \"maxvalue\" , 1 )) minvalue = int ( kwargs . get ( \"minvalue\" , 0 )) base_method = UniformRangeImputer ( maxvalue = maxvalue , minvalue = minvalue ) else : raise ValueError ( f \"Unsupported imputation method: { method } \" ) self . quantitative_data . X = base_method . fit_transform ( self . quantitative_data . X ) return self","title":"impute"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.interpret","text":"Interpret the model's predictions using SHAP values. Parameters: classifier \u2013 The classifier model to interpret. scaler ( optional , default: None ) \u2013 The scaler object to use for data scaling. shap_algorithm ( str , default: 'auto' ) \u2013 The SHAP algorithm to use. Defaults to \"auto\". scale ( bool , default: True ) \u2013 Whether to scale the data before interpretation. Defaults to True. downsample_background ( bool , default: False ) \u2013 Whether to downsample background data. Defaults to False. Returns: QuantMatrix ( QuantMatrix ) \u2013 The QuantMatrix object with SHAP values added to observations. Examples: >>> quant_matrix . interpret ( classifier = clf , scaler = std_scaler ) Source code in dpks/quant_matrix.py 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 def interpret ( self , classifier , scaler : Any = None , shap_algorithm : str = \"auto\" , scale : bool = True , downsample_background = False , ) -> QuantMatrix : \"\"\"Interpret the model's predictions using SHAP values. Args: classifier: The classifier model to interpret. scaler (optional): The scaler object to use for data scaling. shap_algorithm (str): The SHAP algorithm to use. Defaults to \"auto\". scale (bool): Whether to scale the data before interpretation. Defaults to True. downsample_background (bool): Whether to downsample background data. Defaults to False. Returns: QuantMatrix: The QuantMatrix object with SHAP values added to observations. Examples: >>> quant_matrix.interpret(classifier=clf, scaler=std_scaler) \"\"\" X = format_data ( self ) y = encode_labels ( self . quantitative_data . var [ \"group\" ] . values ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) classifier = Classifier ( classifier = classifier , shap_algorithm = shap_algorithm ) if downsample_background : rus = RandomUnderSampler ( random_state = 0 ) X_resampled , y_resampled = rus . fit_resample ( X , y ) classifier . interpret ( X_resampled ) self . transformed_data = X_resampled self . y_resampled = y_resampled else : classifier . interpret ( X ) self . transformed_data = X self . classifier = classifier shap_values = classifier . feature_importances_ . tolist () self . quantitative_data . obs [ \"SHAP\" ] = shap_values self . shap = classifier . shap_values return self","title":"interpret"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.normalize","text":"Normalize the QuantMatrix data. Parameters: method ( str ) \u2013 Normalization method. Options are 'tic', 'median', or 'mean'. log_transform ( bool , default: True ) \u2013 Whether to log-transform the data. Defaults to True. use_rt_sliding_window_filter ( bool , default: False ) \u2013 Whether to use a sliding window filter. Defaults to False. Can only use if a RetentionTime column was loaded in the QuantMatrix **kwargs ( Union [ int , bool , str ] , default: {} ) \u2013 Additional keyword arguments depending on the chosen method. Returns: QuantMatrix ( QuantMatrix ) \u2013 Normalized QuantMatrix object. Raises: ValueError \u2013 If the provided normalization method is not supported. Examples: >>> quant_matrix . normalize ( method = \"mean\" ) Source code in dpks/quant_matrix.py 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def normalize ( self , method : str , log_transform : bool = True , use_rt_sliding_window_filter : bool = False , ** kwargs : Union [ int , bool , str ], ) -> QuantMatrix : \"\"\"Normalize the QuantMatrix data. Args: method (str): Normalization method. Options are 'tic', 'median', or 'mean'. log_transform (bool, optional): Whether to log-transform the data. Defaults to True. use_rt_sliding_window_filter (bool, optional): Whether to use a sliding window filter. Defaults to False. Can only use if a RetentionTime column was loaded in the QuantMatrix **kwargs: Additional keyword arguments depending on the chosen method. Returns: QuantMatrix: Normalized QuantMatrix object. Raises: ValueError: If the provided normalization method is not supported. Examples: >>> quant_matrix.normalize(method=\"mean\") \"\"\" base_method : NormalizationMethod = NormalizationMethod () if method == \"tic\" : base_method = TicNormalization () elif method == \"median\" : base_method = MedianNormalization () elif method == \"mean\" : base_method = MeanNormalization () else : raise ValueError ( f \"Unsupported normalization method: { method } \" ) if use_rt_sliding_window_filter : minimum_data_points = int ( kwargs . get ( \"minimum_data_points\" , 100 )) stride = int ( kwargs . get ( \"stride\" , 1 )) use_overlapping_windows = bool ( kwargs . get ( \"use_overlapping_windows\" , True )) rt_unit = str ( kwargs . get ( \"rt_unit\" , \"minute\" )) rt_window_normalization = RTSlidingWindowNormalization ( base_method = base_method , minimum_data_points = minimum_data_points , stride = stride , use_overlapping_windows = use_overlapping_windows , rt_unit = rt_unit , ) self . quantitative_data . X = rt_window_normalization . fit_transform ( self ) else : self . quantitative_data . X = base_method . fit_transform ( self . quantitative_data . X ) if log_transform : self . quantitative_data . X = Log2Normalization () . fit_transform ( self . quantitative_data . X ) return self","title":"normalize"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.optimize","text":"Optimize hyperparameters of a classifier using different search methods. Parameters: classifier \u2013 The classifier object or class to optimize. param_search_method ( str ) \u2013 The parameter search method to use (\"genetic\" or \"random\"). param_grid ( dict ) \u2013 The parameter grid to search over. scaler ( Any , default: None ) \u2013 The scaler object to scale the data. Defaults to None. scale ( bool , default: True ) \u2013 Whether to scale the data. Defaults to True. threads ( int , default: 1 ) \u2013 The number of threads to use for optimization. Defaults to 1. random_state ( int , default: 42 ) \u2013 Random seed for reproducibility. Defaults to 42. folds ( int , default: 3 ) \u2013 The number of folds for cross-validation. Defaults to 3. verbose ( Union [ bool , int ] , default: False ) \u2013 Verbosity level. Defaults to False. **kwargs ( Union [ dict , int , str , bool ] , default: {} ) \u2013 Additional keyword arguments specific to each search method. Returns: ParamSearchResult ( ParamSearchResult ) \u2013 The result of the parameter search, including the best estimator and parameter populations. Examples: >>> param_grid = { 'max_depth' : [ 3 , 5 , 7 ], 'min_samples_split' : [ 2 , 5 , 10 ]} >>> result = quant_matrix . optimize ( classifier = DecisionTreeClassifier (), param_search_method = 'random' , param_grid = param_grid , verbose = True ) >>> result . best_estimator_ DecisionTreeClassifier(max_depth=5, min_samples_split=10) Source code in dpks/quant_matrix.py 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 def optimize ( self , classifier , param_search_method : str , param_grid : dict , scaler : Any = None , scale : bool = True , threads : int = 1 , random_state : int = 42 , folds : int = 3 , verbose : Union [ bool , int ] = False , ** kwargs : Union [ dict , int , str , bool ], ) -> ParamSearchResult : \"\"\"Optimize hyperparameters of a classifier using different search methods. Args: classifier: The classifier object or class to optimize. param_search_method (str): The parameter search method to use (\"genetic\" or \"random\"). param_grid (dict): The parameter grid to search over. scaler (Any): The scaler object to scale the data. Defaults to None. scale (bool): Whether to scale the data. Defaults to True. threads (int): The number of threads to use for optimization. Defaults to 1. random_state (int): Random seed for reproducibility. Defaults to 42. folds (int): The number of folds for cross-validation. Defaults to 3. verbose (Union[bool, int]): Verbosity level. Defaults to False. **kwargs: Additional keyword arguments specific to each search method. Returns: ParamSearchResult: The result of the parameter search, including the best estimator and parameter populations. Examples: >>> param_grid = {'max_depth': [3, 5, 7], 'min_samples_split': [2, 5, 10]} >>> result = quant_matrix.optimize(classifier=DecisionTreeClassifier(), param_search_method='random', param_grid=param_grid, verbose=True) >>> result.best_estimator_ DecisionTreeClassifier(max_depth=5, min_samples_split=10) \"\"\" X = format_data ( self ) y = encode_labels ( self . quantitative_data . var [ \"group\" ] . values ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) result = None if param_search_method == \"genetic\" : gas = GeneticAlgorithmSearch ( classifier , param_grid = param_grid , threads = threads , folds = folds , n_survive = kwargs . get ( \"n_survive\" , 5 ), pop_size = kwargs . get ( \"pop_size\" , 10 ), n_generations = kwargs . get ( \"n_generations\" , 20 ), verbose = verbose , random_state = kwargs . get ( \"random_state\" , None ), shuffle = kwargs . get ( \"shuffle\" , False ), ) parameter_populations = gas . fit ( X , y ) result = ParamSearchResult ( classifier = gas . best_estimator_ , result = parameter_populations , ) elif param_search_method == \"random\" : randomized_search = RandomizedSearch ( classifier , param_grid = param_grid , folds = folds , random_state = random_state , n_iter = kwargs . get ( \"n_iter\" , 30 ), n_jobs = threads , scoring = kwargs . get ( \"scoring\" , \"accuracy\" ), verbose = verbose , ) result = randomized_search . fit ( X , y ) return result","title":"optimize"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.plot","text":"Generate plots based on specified plot type. Parameters: plot_type ( str ) \u2013 The type of plot to generate. Possible values are: - \"shap_summary\": SHAP summary plot. - \"rfe_pca\": Recursive Feature Elimination (RFE) with Principal Component Analysis (PCA) plot. save ( bool , default: False ) \u2013 Whether to save the plot. Defaults to False. fig ( Figure , default: None ) \u2013 The matplotlib figure object. Defaults to None. ax ( Union [ list , Axes ] , default: None ) \u2013 The list of matplotlib axes objects or a single axes object. Defaults to None. **kwargs ( Union [ ndarray , int , list , str ] , default: {} ) \u2013 Additional keyword arguments specific to each plot type. Returns: tuple [ Figure , Axes ] \u2013 tuple[matplotlib.figure.Figure, matplotlib.axes.Axes]: The matplotlib figure and axes objects. Raises: ValueError \u2013 If an unsupported plot type is provided. Examples: >>> fig , ax = quant_matrix . plot ( plot_type = 'shap_summary' , save = True , n_display = 10 ) Source code in dpks/quant_matrix.py 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 def plot ( self , plot_type : str , save : bool = False , fig : matplotlib . figure . Figure = None , ax : Union [ list , matplotlib . axes . Axes ] = None , ** kwargs : Union [ np . ndarray , int , list , str , ], ) -> tuple [ matplotlib . figure . Figure , matplotlib . axes . Axes ]: \"\"\"Generate plots based on specified plot type. Args: plot_type (str): The type of plot to generate. Possible values are: - \"shap_summary\": SHAP summary plot. - \"rfe_pca\": Recursive Feature Elimination (RFE) with Principal Component Analysis (PCA) plot. save (bool): Whether to save the plot. Defaults to False. fig (matplotlib.figure.Figure): The matplotlib figure object. Defaults to None. ax (Union[list, matplotlib.axes.Axes]): The list of matplotlib axes objects or a single axes object. Defaults to None. **kwargs: Additional keyword arguments specific to each plot type. Returns: tuple[matplotlib.figure.Figure, matplotlib.axes.Axes]: The matplotlib figure and axes objects. Raises: ValueError: If an unsupported plot type is provided. Examples: >>> fig, ax = quant_matrix.plot(plot_type='shap_summary', save=True, n_display=10) \"\"\" if plot_type == \"shap_summary\" : try : getattr ( self , \"shap\" ) except AttributeError : print ( \"SHAP values have not been generated\" ) cmap = kwargs . get ( \"cmap\" , [ \"#ff4800\" , \"#ff4040\" , \"#a836ff\" , \"#405cff\" , \"#05c9fa\" , ], ) order_by = kwargs . get ( \"order_by\" , \"shap\" ) fig , ax = SHAPPlot ( fig = fig , ax = ax , shap_values = self . shap , X = self . transformed_data , qm = self , cmap = cmap , n_display = kwargs . get ( \"n_display\" , 5 ), jitter = kwargs . get ( \"jitter\" , 0.1 ), alpha = kwargs . get ( \"alpha\" , 0.75 ), n_bins = kwargs . get ( \"n_bins\" , 100 ), feature_column = kwargs . get ( \"feature_column\" , \"Protein\" ), order_by = order_by , ) . plot () if plot_type == \"rfe_pca\" : cmap = kwargs . get ( \"cmap\" , \"coolwarm\" ) cutoffs = list ( kwargs . get ( \"cutoffs\" , [ 100 , 50 , 10 ])) fig , ax = RFEPCA ( fig = fig , axs = ax , qm = self , cutoffs = cutoffs , cmap = cmap ) . plot () if save : filepath = str ( kwargs . get ( \"filepath\" , f \" { plot_type } .png\" )) dpi = int ( kwargs . get ( \"dpi\" , 300 )) matplotlib . pyplot . savefig ( filepath , dpi = dpi ) return fig , ax","title":"plot"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.predict","text":"Predict labels using a classifier. Parameters: classifier \u2013 The classifier model to use for prediction. scaler ( optional , default: None ) \u2013 The scaler object to use for data scaling. scale ( bool , default: True ) \u2013 Whether to scale the data before prediction. Defaults to True. Returns: QuantMatrix ( QuantMatrix ) \u2013 The QuantMatrix object with predicted labels. Examples: >>> quant_matrix . predict ( classifier = clf , scaler = std_scaler ) Source code in dpks/quant_matrix.py 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 def predict ( self , classifier , scaler : Any = None , scale : bool = True , ) -> QuantMatrix : \"\"\"Predict labels using a classifier. Args: classifier: The classifier model to use for prediction. scaler (optional): The scaler object to use for data scaling. scale (bool): Whether to scale the data before prediction. Defaults to True. Returns: QuantMatrix: The QuantMatrix object with predicted labels. Examples: >>> quant_matrix.predict(classifier=clf, scaler=std_scaler) \"\"\" X = format_data ( self ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) classifier = Classifier ( classifier = classifier ) self . sample_annotations [ \"Prediction\" ] = classifier . predict ( X ) return self","title":"predict"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.quantify","text":"Calculate protein quantities. Parameters: method ( str ) \u2013 Quantification method. Options are 'top_n' or 'maxlfq'. **kwargs ( Union [ int , str ] , default: {} ) \u2013 Additional keyword arguments depending on the chosen method. Returns: QuantMatrix ( QuantMatrix ) \u2013 Quantified protein matrix. Raises: ValueError \u2013 If the provided quantification method is not supported. Examples: >>> quant_matrix . quantify ( method = \"top_n\" , top_n = 1 ) Source code in dpks/quant_matrix.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def quantify ( self , method : str , ** kwargs : Union [ int , str ], ) -> QuantMatrix : \"\"\"Calculate protein quantities. Args: method (str): Quantification method. Options are 'top_n' or 'maxlfq'. **kwargs: Additional keyword arguments depending on the chosen method. Returns: QuantMatrix: Quantified protein matrix. Raises: ValueError: If the provided quantification method is not supported. Examples: >>> quant_matrix.quantify(method=\"top_n\", top_n=1) \"\"\" if method == \"top_n\" : level = str ( kwargs . get ( \"level\" , \"protein\" )) top_n = int ( kwargs . get ( \"top_n\" , 1 )) summarization_method = str ( kwargs . get ( \"summarization_method\" , \"sum\" )) quantifications = TopN ( top_n = top_n , level = level , summarization_method = summarization_method ) . quantify ( self ) design_matrix = self . quantitative_data . var protein_quantifications = QuantMatrix ( quantifications , design_matrix_file = design_matrix ) elif method == \"maxlfq\" : level = str ( kwargs . get ( \"level\" , \"protein\" )) threads = int ( kwargs . get ( \"threads\" , 1 )) minimum_subgroups = int ( kwargs . get ( \"minimum_subgroups\" , 1 )) top_n = int ( kwargs . get ( \"top_n\" , 0 )) quantifications = MaxLFQ ( level = level , threads = threads , minimum_subgroups = minimum_subgroups , top_n = top_n , ) . quantify ( self ) design_matrix = self . quantitative_data . var protein_quantifications = QuantMatrix ( quantifications , design_matrix_file = design_matrix ) else : raise ValueError ( f \"Unsupported quantification method: { method } \" ) return protein_quantifications","title":"quantify"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.scale","text":"Scale the QuantMatrix data at the feature level (i.e Precursor or Protein). Parameters: method ( str ) \u2013 Scaling method. Options are 'zscore', 'minmax', or 'absmax'. Returns: QuantMatrix ( QuantMatrix ) \u2013 Scaled QuantMatrix object. Raises: ValueError \u2013 If the provided scaling method is not supported. Source code in dpks/quant_matrix.py 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 def scale ( self , method : str , ) -> QuantMatrix : \"\"\"Scale the QuantMatrix data at the feature level (i.e Precursor or Protein). Args: method (str): Scaling method. Options are 'zscore', 'minmax', or 'absmax'. Returns: QuantMatrix: Scaled QuantMatrix object. Raises: ValueError: If the provided scaling method is not supported. \"\"\" base_method : ScalingMethod = ScalingMethod () if method == \"zscore\" : base_method = ZScoreScaling () elif method == \"minmax\" : base_method = MinMaxScaling () elif method == \"absmax\" : base_method = AbsMaxScaling () else : raise ValueError ( f \"Unsupported scaling method: { method } \" ) self . quantitative_data . X = base_method . fit_transform ( self . quantitative_data . X ) return self","title":"scale"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.to_df","text":"Convert the QuantMatrix object to a pandas DataFrame. Returns: DataFrame \u2013 pd.DataFrame: DataFrame representation of the QuantMatrix. Examples: >>> quant_matrix . to_df () Source code in dpks/quant_matrix.py 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 def to_df ( self ) -> pd . DataFrame : \"\"\"Convert the QuantMatrix object to a pandas DataFrame. Returns: pd.DataFrame: DataFrame representation of the QuantMatrix. Examples: >>> quant_matrix.to_df() \"\"\" quant_data = self . quantitative_data [ self . row_annotations . index , :] . to_df () merged = pd . concat ([ self . row_annotations , quant_data ], axis = 1 ) return merged","title":"to_df"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.to_ml","text":"Converts the QuantMatrix object to features and labels for machine learning. Parameters: feature_column ( str , default: 'Protein' ) \u2013 The column to use as features. Defaults to \"Protein\". label_column ( str , default: 'group' ) \u2013 The column to use as labels. Defaults to \"group\". comparison ( tuple , default: (1, 2) ) \u2013 The comparison groups. Defaults to (1, 2). Returns: tuple [ Any , Any ] \u2013 tuple[Any, Any]: A tuple containing features and labels. Examples: >>> features , labels = quant_matrix . to_ml () Source code in dpks/quant_matrix.py 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 def to_ml ( self , feature_column : str = \"Protein\" , label_column : str = \"group\" , comparison : tuple = ( 1 , 2 ), ) -> tuple [ Any , Any ]: \"\"\"Converts the QuantMatrix object to features and labels for machine learning. Args: feature_column (str, optional): The column to use as features. Defaults to \"Protein\". label_column (str, optional): The column to use as labels. Defaults to \"group\". comparison (tuple, optional): The comparison groups. Defaults to (1, 2). Returns: tuple[Any, Any]: A tuple containing features and labels. Examples: >>> features, labels = quant_matrix.to_ml() \"\"\" qm_df = self . to_df () samples = self . sample_annotations [ self . sample_annotations [ \"group\" ] . isin ( comparison ) ][ \"sample\" ] . to_list () transposed_features = qm_df . set_index ( feature_column )[ samples ] . T sample_annotations = self . sample_annotations . copy () sample_annotations_subset = sample_annotations [ sample_annotations [ label_column ] . isin ( comparison ) ] . copy () encoder = LabelEncoder () sample_annotations_subset [ \"label\" ] = encoder . fit_transform ( sample_annotations_subset [ label_column ] ) combined = transposed_features . join ( sample_annotations_subset [[ \"sample\" , \"label\" ]] . set_index ( \"sample\" ), how = \"left\" , ) return combined . loc [:, combined . columns != \"label\" ], combined [[ \"label\" ]]","title":"to_ml"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.train","text":"Train a classifier on the quantitative data. Parameters: classifier \u2013 The classifier object or class to use for training. scaler ( Any , default: None ) \u2013 The scaler object to scale the data. Defaults to None. scale ( bool , default: True ) \u2013 Whether to scale the data. Defaults to True. validate ( bool , default: True ) \u2013 Whether to perform cross-validation. Defaults to True. scoring ( str , default: 'accuracy' ) \u2013 The scoring metric for cross-validation. Defaults to \"accuracy\". num_folds ( int , default: 3 ) \u2013 The number of folds for cross-validation. Defaults to 3. random_state ( int , default: 42 ) \u2013 Random seed for reproducibility. Defaults to 42. shuffle ( bool , default: False ) \u2013 Whether to shuffle the data before splitting in cross-validation. Defaults to False. Returns: TrainResult ( TrainResult ) \u2013 The result of the training process, including the trained classifier, scaler, and validation scores. Examples: >>> result = quant_matrix . train ( classifier = RandomForestClassifier (), validate = True ) Source code in dpks/quant_matrix.py 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 def train ( self , classifier , scaler : Any = None , scale : bool = True , validate : bool = True , scoring : str = \"accuracy\" , num_folds : int = 3 , random_state : int = 42 , shuffle : bool = False , ) -> TrainResult : \"\"\"Train a classifier on the quantitative data. Args: classifier: The classifier object or class to use for training. scaler (Any): The scaler object to scale the data. Defaults to None. scale (bool): Whether to scale the data. Defaults to True. validate (bool): Whether to perform cross-validation. Defaults to True. scoring (str): The scoring metric for cross-validation. Defaults to \"accuracy\". num_folds (int): The number of folds for cross-validation. Defaults to 3. random_state (int): Random seed for reproducibility. Defaults to 42. shuffle (bool): Whether to shuffle the data before splitting in cross-validation. Defaults to False. Returns: TrainResult: The result of the training process, including the trained classifier, scaler, and validation scores. Examples: >>> result = quant_matrix.train(classifier=RandomForestClassifier(), validate=True) \"\"\" X = format_data ( self ) y = encode_labels ( self . quantitative_data . var [ \"group\" ] . values ) if scale : if scaler : X = scaler . transform ( X ) else : scaler = StandardScaler () X = scaler . fit_transform ( X ) classifier = Classifier ( classifier = classifier ) validation_result = np . array ([]) if validate : cv = StratifiedKFold ( num_folds , shuffle = shuffle , random_state = random_state ) validation_result = cross_val_score ( classifier , X , y , scoring = scoring , cv = cv ) classifier . fit ( X , y ) return TrainResult ( classifier , scaler , validation_result )","title":"train"},{"location":"reference/quant_matrix_ref/#dpks.quant_matrix.QuantMatrix.write","text":"Write the QuantMatrix to a tab-separated file. Parameters: file_path ( str ) \u2013 The path where the file will be saved. Returns: None \u2013 None Examples: >>> filename = \"protein.tsv\" >>> quant_matrix . write ( filename ) Source code in dpks/quant_matrix.py 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 def write ( self , file_path : str ) -> None : \"\"\"Write the QuantMatrix to a tab-separated file. Args: file_path (str): The path where the file will be saved. Returns: None Examples: >>> filename = \"protein.tsv\" >>> quant_matrix.write(filename) \"\"\" self . to_df () . to_csv ( file_path , sep = \" \\t \" , index = False )","title":"write"},{"location":"usage/explainable_machine_learning/","text":"Explainable Machine Learning Sometimes when you are comparing experimental groups, it could be useful to identify only the most important proteins associated with a particular proteome state. By selecting panels of proteins using machine learning, you can accelerate the process of biomarker discovery, learn more about your dysregulated proteomes of interest, select important proteins from the noise for further analysis. Sometimes, it is enough to select the most statistically significant differentially abundant proteins, but many times these proteins may just be generic markers of inflammation in disease, but not specific to a particular disease, or the most important in classifying between 2 proteome states. Using DPKS, we can apply advanced explainable machine learning techniques using our bootstrapping approach combined with SHAP to calculate mean importances and mean ranks for proteins in an experiment to help you decide which proteins to focus on for further analysis. From a QuantMatrix object, it is trivial to apply these methods to your data: from dpks import QuantMatrix from xgboost import XGBClassifier clf = XGBClassifier( max_depth=2, reg_lambda=2, objective=\"binary:logistic\", seed=42 ) qm = qm.explain( clf, comparisons=[(2, 1), (3, 1)], n_iterations=10, downsample_background=True ) You just need to pass in a classifier of your choice, and indicate the comparisons you want to perform. It is possible to perform multiple explanations, just like with differential abundance analysis. The output columns will be named MeanSHAP2-1 or MeanSHAP3-1 and MeanRank2-1 or MeanRank3-1 depending on the number of comparisons passed in to explain() . Warning If you pass in a classifier other than XGBCLassifier or another tree based method, the SHAP value calulation will be extremely slow, so it is best to use XGBClassifier for now. We are working on adding fast support for linear methods without using SHAP. From here, it is straightforward to visualize which proteins are most important for predicting a particular proteome state. Example There is a jupyter notebook with some examples of how to use this functionality and some possible plots. Explainable Machine Learning : Demonstrates how to compute feature importance for proteins when predicting some experimental conditions.","title":"Explainable Machine Learning"},{"location":"usage/explainable_machine_learning/#explainable-machine-learning","text":"Sometimes when you are comparing experimental groups, it could be useful to identify only the most important proteins associated with a particular proteome state. By selecting panels of proteins using machine learning, you can accelerate the process of biomarker discovery, learn more about your dysregulated proteomes of interest, select important proteins from the noise for further analysis. Sometimes, it is enough to select the most statistically significant differentially abundant proteins, but many times these proteins may just be generic markers of inflammation in disease, but not specific to a particular disease, or the most important in classifying between 2 proteome states. Using DPKS, we can apply advanced explainable machine learning techniques using our bootstrapping approach combined with SHAP to calculate mean importances and mean ranks for proteins in an experiment to help you decide which proteins to focus on for further analysis. From a QuantMatrix object, it is trivial to apply these methods to your data: from dpks import QuantMatrix from xgboost import XGBClassifier clf = XGBClassifier( max_depth=2, reg_lambda=2, objective=\"binary:logistic\", seed=42 ) qm = qm.explain( clf, comparisons=[(2, 1), (3, 1)], n_iterations=10, downsample_background=True ) You just need to pass in a classifier of your choice, and indicate the comparisons you want to perform. It is possible to perform multiple explanations, just like with differential abundance analysis. The output columns will be named MeanSHAP2-1 or MeanSHAP3-1 and MeanRank2-1 or MeanRank3-1 depending on the number of comparisons passed in to explain() . Warning If you pass in a classifier other than XGBCLassifier or another tree based method, the SHAP value calulation will be extremely slow, so it is best to use XGBClassifier for now. We are working on adding fast support for linear methods without using SHAP. From here, it is straightforward to visualize which proteins are most important for predicting a particular proteome state.","title":"Explainable Machine Learning"},{"location":"usage/explainable_machine_learning/#example","text":"There is a jupyter notebook with some examples of how to use this functionality and some possible plots. Explainable Machine Learning : Demonstrates how to compute feature importance for proteins when predicting some experimental conditions.","title":"Example"},{"location":"usage/installation/","text":"Installation Install DPKS The sources for dpks can be downloaded from the Github repo . You can clone the repository: git clone git://github.com/InfectionMedicineProteomics/DPKS To install dpks, run this command in your terminal: cd DPKS && pip install . If you are developing DPKS, run this command in your terminal: pip install tox flake8 flake8-html coverage pytest pytest-html pytest-cov black mypy bandit cd DPKS && pip install -e . If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Installation"},{"location":"usage/installation/#installation","text":"","title":"Installation"},{"location":"usage/installation/#install-dpks","text":"The sources for dpks can be downloaded from the Github repo . You can clone the repository: git clone git://github.com/InfectionMedicineProteomics/DPKS To install dpks, run this command in your terminal: cd DPKS && pip install . If you are developing DPKS, run this command in your terminal: pip install tox flake8 flake8-html coverage pytest pytest-html pytest-cov black mypy bandit cd DPKS && pip install -e . If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Install DPKS"},{"location":"usage/loading_data/","text":"Loading Data DPKS can load data from a variety of different proteomic processing pipelines directly. If you have a filetype that you would like to be able to parse directly into DPKS, please let us know. The QuantMatrix is the main entry point to all analysis in DPKS. A new QuantMatrix object can be instantiated with your input data and a design matrix by passing the file paths: quant_matrix = QuantMatrix( quantification_file=\"path_to_quant_file.tsv\", design_matrix_file=\"path_to_design_matrix_file.tsv\" ) Or by passing in a pandas DataFrame : quant_data = pd.read_csv( \"path_to_quant_file.tsv\", sep=\"\\t\" ) design_matrix = pd.read_csv( \"path_to_design_matrix_file.tsv\", sep=\"\\t\" ) quant_matrix = QuantMatrix( quantification_file=quant_data, design_matrix_file=design_matrix ) This is particularly useful if you want to process your data (reformat, filter, etc.) in someway before loading it into DPKS. The ability to pass in files or DataFrames directly to the QuantMatrix object provides some flexibility in the type of data that you can load, making it easy to write custom parsers for new result file types. Tip If you encounter errors during parsing, it is useful to first load your data as DataFrame s to first verify that everything is formatted correctly Generic Input Quantitative Data DPKS accepts a generic results file that you can reformat your own data to if there is not a built-in parser available. Column Description PrecursorId A unique identifier generally composed of the Peptide Sequence (with mods) and the charge. Charge The precursor charge. PeptideSequence The modified peptide sequence. Decoy (Optional) Indicating if the precursor is a decoy (used for filtering). RetentionTime The retention time of the precursor. Protein The protein accession code associated with the precursor. PeptideQValue (Optional) The global peptide level q-value (used for filtering). ProteinQValue (Optional) The global protein level q-value (used for filtering). Sample Columns (Many Columns) All other columns containing quantification data for your samples. If you already have controlled for the global FDR, you do not need to include the Decoy, PeptideQValue, or ProteinQValue columns. A generic file format may look like this: PeptideSequence Charge Decoy Protein RetentionTime PeptideQValue ProteinQValue SAMPLE_1.osw SAMPLE_2.osw SAMPLE_3.osw PEPTIK 4 0 P00352 5736.15 7.81e-06 0.0001169 29566.2 59295.7 24536.4 EFMEEVIQR 2 0 P04275 3155.5 9.41e-06 0.0001169 69900.3 195571.0 403947.0 SSSGTPDLPVLLTDLK 2 0 P00352 5386.69 7.815e-06 0.000116 115684.0 132524.0 217962.0 Note If you want to pass already quantified Proteins you could do this: Protein SAMPLE_1.osw SAMPLE_2.osw SAMPLE_3.osw P00352 29566.2 59295.7 24536.4 P04275 69900.3 195571.0 403947.0 P00352 115684.0 132524.0 217962.0 Design Matrix A basic design matrix will have 2 main columns: Column Description Sample (Required) A list of the samples. This helps differentiate between sample columns and annotation columns in the QuantMatrix Group (Optional) The group the sample belongs to. Used in differential testing and explainable machine learning. A minimal design matrix for the above input examples could look like this: Sample SAMPLE_1.osw SAMPLE_2.osw SAMPLE_3.osw And an example using the Group column: sample group AAS_P2009_167 6 AAS_P2009_169 4 AAS_P2009_176 6 AAS_P2009_178 4 AAS_P2009_187 4 AAS_P2009_194 6 AAS_P2009_196 4 AAS_P2009_203 6 AAS_P2009_205 4 AAS_P2009_212 6 AAS_P2009_214 4 AAS_P2009_221 6 AAS_P2009_230 6 AAS_P2009_232 4 AAS_P2009_239 6 AAS_P2009_241 4 AAS_P2009_248 6 AAS_P2009_250 4 DIANN You can load data directly from DIA-NN using the long-format diann-output.tsv file that is generated. The samples in your design matrix column should match the Run column in the DIA-NN output, but other columns can be indicated if desired. Additionally, if you have used MBR, the correct columns will be used to filter precursors at the indicated FDR threshold. quant_matrix = QuantMatrix( quantification_file=quant_file, design_matrix_file=simple_design, quant_type=\"diann\", diann_qvalue=0.01 )","title":"Loading data"},{"location":"usage/loading_data/#loading-data","text":"DPKS can load data from a variety of different proteomic processing pipelines directly. If you have a filetype that you would like to be able to parse directly into DPKS, please let us know. The QuantMatrix is the main entry point to all analysis in DPKS. A new QuantMatrix object can be instantiated with your input data and a design matrix by passing the file paths: quant_matrix = QuantMatrix( quantification_file=\"path_to_quant_file.tsv\", design_matrix_file=\"path_to_design_matrix_file.tsv\" ) Or by passing in a pandas DataFrame : quant_data = pd.read_csv( \"path_to_quant_file.tsv\", sep=\"\\t\" ) design_matrix = pd.read_csv( \"path_to_design_matrix_file.tsv\", sep=\"\\t\" ) quant_matrix = QuantMatrix( quantification_file=quant_data, design_matrix_file=design_matrix ) This is particularly useful if you want to process your data (reformat, filter, etc.) in someway before loading it into DPKS. The ability to pass in files or DataFrames directly to the QuantMatrix object provides some flexibility in the type of data that you can load, making it easy to write custom parsers for new result file types. Tip If you encounter errors during parsing, it is useful to first load your data as DataFrame s to first verify that everything is formatted correctly","title":"Loading Data"},{"location":"usage/loading_data/#generic-input","text":"","title":"Generic Input"},{"location":"usage/loading_data/#quantitative-data","text":"DPKS accepts a generic results file that you can reformat your own data to if there is not a built-in parser available. Column Description PrecursorId A unique identifier generally composed of the Peptide Sequence (with mods) and the charge. Charge The precursor charge. PeptideSequence The modified peptide sequence. Decoy (Optional) Indicating if the precursor is a decoy (used for filtering). RetentionTime The retention time of the precursor. Protein The protein accession code associated with the precursor. PeptideQValue (Optional) The global peptide level q-value (used for filtering). ProteinQValue (Optional) The global protein level q-value (used for filtering). Sample Columns (Many Columns) All other columns containing quantification data for your samples. If you already have controlled for the global FDR, you do not need to include the Decoy, PeptideQValue, or ProteinQValue columns. A generic file format may look like this: PeptideSequence Charge Decoy Protein RetentionTime PeptideQValue ProteinQValue SAMPLE_1.osw SAMPLE_2.osw SAMPLE_3.osw PEPTIK 4 0 P00352 5736.15 7.81e-06 0.0001169 29566.2 59295.7 24536.4 EFMEEVIQR 2 0 P04275 3155.5 9.41e-06 0.0001169 69900.3 195571.0 403947.0 SSSGTPDLPVLLTDLK 2 0 P00352 5386.69 7.815e-06 0.000116 115684.0 132524.0 217962.0 Note If you want to pass already quantified Proteins you could do this: Protein SAMPLE_1.osw SAMPLE_2.osw SAMPLE_3.osw P00352 29566.2 59295.7 24536.4 P04275 69900.3 195571.0 403947.0 P00352 115684.0 132524.0 217962.0","title":"Quantitative Data"},{"location":"usage/loading_data/#design-matrix","text":"A basic design matrix will have 2 main columns: Column Description Sample (Required) A list of the samples. This helps differentiate between sample columns and annotation columns in the QuantMatrix Group (Optional) The group the sample belongs to. Used in differential testing and explainable machine learning. A minimal design matrix for the above input examples could look like this: Sample SAMPLE_1.osw SAMPLE_2.osw SAMPLE_3.osw And an example using the Group column: sample group AAS_P2009_167 6 AAS_P2009_169 4 AAS_P2009_176 6 AAS_P2009_178 4 AAS_P2009_187 4 AAS_P2009_194 6 AAS_P2009_196 4 AAS_P2009_203 6 AAS_P2009_205 4 AAS_P2009_212 6 AAS_P2009_214 4 AAS_P2009_221 6 AAS_P2009_230 6 AAS_P2009_232 4 AAS_P2009_239 6 AAS_P2009_241 4 AAS_P2009_248 6 AAS_P2009_250 4","title":"Design Matrix"},{"location":"usage/loading_data/#diann","text":"You can load data directly from DIA-NN using the long-format diann-output.tsv file that is generated. The samples in your design matrix column should match the Run column in the DIA-NN output, but other columns can be indicated if desired. Additionally, if you have used MBR, the correct columns will be used to filter precursors at the indicated FDR threshold. quant_matrix = QuantMatrix( quantification_file=quant_file, design_matrix_file=simple_design, quant_type=\"diann\", diann_qvalue=0.01 )","title":"DIANN"},{"location":"usage/normalization/","text":"Normalization Normalization a standard method used in omic analysis to minimize batch effect and technical variation between samples prior to further downstream analysis. This is particularly important to ensure that the signal you are seeing in your analysis is actual biological signal and not just noise. DPKS exposes 3 main methods for normalizing your quantitative matrices: tic - Total ion chromatogram normalization: The sum of all signal in each sample is used to scale each measured intensity, and the median of the sums is used to rescale the data. median - Median normalization: The median of each sample is used to scale each intensity and the mean of the medians is used to rescale the data. mean - Mean normalization: The mean of each sample is used to scale each intensity and the mean of the means is used to rescale the data. Tip An implementation of retention time sliding window normalization 1 can be applied if your input data was LC-MS/MS data and a RetentionTime column is available. This is recommended. The data is then log2 transformed to allow for interpretable downstream analysis. Note You should always (generally) log2 transform your data. Especially if you are doing differential expression analysis. This is a default in DPKS, so you do not need to worry about this. Normalization can be performed as follows: from dpks.quant_matrix import QuantMatrix qm = QuantMatrix( quantification_file=\"../tests/input_files/de_matrix.tsv\", design_matrix_file=\"../tests/input_files/de_design_matrix.tsv\" ).filter() qm = qm.normalize( method=\"mean\", log_transform=True, use_rt_sliding_window_filter=True, minimum_data_points=100, stride=5, use_overlapping_windows=True, rt_unit=\"seconds\" ) Many of the set paramters above are defaults and do not need to be passed everytime. Jakob Willforss, Aakash Chawade, and Fredrik Levander. NormalyzerDE: Online Tool for Improved Normalization of Omics Expression Data and High-Sensitivity Differential Expression Analysis. Journal of Proteome Research 2019 18 (2), 732-740 DOI: 10.1021/acs.jproteome.8b00523 \u21a9","title":"Normalization"},{"location":"usage/normalization/#normalization","text":"Normalization a standard method used in omic analysis to minimize batch effect and technical variation between samples prior to further downstream analysis. This is particularly important to ensure that the signal you are seeing in your analysis is actual biological signal and not just noise. DPKS exposes 3 main methods for normalizing your quantitative matrices: tic - Total ion chromatogram normalization: The sum of all signal in each sample is used to scale each measured intensity, and the median of the sums is used to rescale the data. median - Median normalization: The median of each sample is used to scale each intensity and the mean of the medians is used to rescale the data. mean - Mean normalization: The mean of each sample is used to scale each intensity and the mean of the means is used to rescale the data. Tip An implementation of retention time sliding window normalization 1 can be applied if your input data was LC-MS/MS data and a RetentionTime column is available. This is recommended. The data is then log2 transformed to allow for interpretable downstream analysis. Note You should always (generally) log2 transform your data. Especially if you are doing differential expression analysis. This is a default in DPKS, so you do not need to worry about this. Normalization can be performed as follows: from dpks.quant_matrix import QuantMatrix qm = QuantMatrix( quantification_file=\"../tests/input_files/de_matrix.tsv\", design_matrix_file=\"../tests/input_files/de_design_matrix.tsv\" ).filter() qm = qm.normalize( method=\"mean\", log_transform=True, use_rt_sliding_window_filter=True, minimum_data_points=100, stride=5, use_overlapping_windows=True, rt_unit=\"seconds\" ) Many of the set paramters above are defaults and do not need to be passed everytime. Jakob Willforss, Aakash Chawade, and Fredrik Levander. NormalyzerDE: Online Tool for Improved Normalization of Omics Expression Data and High-Sensitivity Differential Expression Analysis. Journal of Proteome Research 2019 18 (2), 732-740 DOI: 10.1021/acs.jproteome.8b00523 \u21a9","title":"Normalization"},{"location":"usage/pathway_analysis/","text":"Pathway Analysis Using DPKS it is possible to perform pathway analysis using proteins selected from differential abundance analysis or by using explainable machine learning. We use the methods available in the awesome GSEAPY 1 package to perform overrepresentation tests using hypergeometric distributions on the protein in your QuantMatrix . Note This is one of the few methods in a QuantMatrix that does not return an instance of itself to allow for method chaining. Instead, the results of the overrepresentation tests from GSEAPY are returned. Info We are working on adding support for GSEA analysis directly from a QuantMatrix Basic usage of the enrich() method is as follows: By default, the GO_Biological_Process_2023 is searched if none are indicated using the libraries parameter. For filtering at the FDR level after differential abundance analysis: enr = qm.enrich( method=\"overreptest\", filter_pvalue=True, pvalue_column=\"CorrectedPValue2-1\", pvalue_cutoff=0.1 ) For filtering SHAP values that have some contribution to prediction: enr = qm.enrich( method=\"overreptest\", filter_shap=True, shap_column=\"MeanSHAP2-1\", shap_cutoff=0.0 ) You can also search multiple libraries and subset the databases to only consider pathways that contain the proteins in your QuantMatrix . All pathways available in GSEAPY can be searched. Tip If you have a small number of proteins you are interested in, subsetting the library using the subset_library parameter can be very helpful. This can help with FDR control so that the search space is not too large. enr = quantified_data.enrich( method=\"enrichr_overreptest\", libraries=['GO_Biological_Process_2023', 'KEGG_2021_Human', 'Reactome_2022'], organism=\"human\", filter_pvalue=True, subset_library=True ) The above enr results are detailed in the GSEAPY documentation, and can be used to easily make pathway plots and perform network analysis. Example There is a jupyter notebook with some detailed examples of how to use this functionality and some possible plots. Pathway Enrichment : Demonstrates how to perform pathway enrichment analysis. Zhuoqing Fang, Xinyuan Liu, Gary Peltz, GSEApy: a comprehensive package for performing gene set enrichment analysis in Python, Bioinformatics, 2022;, btac757, https://doi.org/10.1093/bioinformatics/btac757 \u21a9","title":"Pathway Analysis"},{"location":"usage/pathway_analysis/#pathway-analysis","text":"Using DPKS it is possible to perform pathway analysis using proteins selected from differential abundance analysis or by using explainable machine learning. We use the methods available in the awesome GSEAPY 1 package to perform overrepresentation tests using hypergeometric distributions on the protein in your QuantMatrix . Note This is one of the few methods in a QuantMatrix that does not return an instance of itself to allow for method chaining. Instead, the results of the overrepresentation tests from GSEAPY are returned. Info We are working on adding support for GSEA analysis directly from a QuantMatrix Basic usage of the enrich() method is as follows: By default, the GO_Biological_Process_2023 is searched if none are indicated using the libraries parameter. For filtering at the FDR level after differential abundance analysis: enr = qm.enrich( method=\"overreptest\", filter_pvalue=True, pvalue_column=\"CorrectedPValue2-1\", pvalue_cutoff=0.1 ) For filtering SHAP values that have some contribution to prediction: enr = qm.enrich( method=\"overreptest\", filter_shap=True, shap_column=\"MeanSHAP2-1\", shap_cutoff=0.0 ) You can also search multiple libraries and subset the databases to only consider pathways that contain the proteins in your QuantMatrix . All pathways available in GSEAPY can be searched. Tip If you have a small number of proteins you are interested in, subsetting the library using the subset_library parameter can be very helpful. This can help with FDR control so that the search space is not too large. enr = quantified_data.enrich( method=\"enrichr_overreptest\", libraries=['GO_Biological_Process_2023', 'KEGG_2021_Human', 'Reactome_2022'], organism=\"human\", filter_pvalue=True, subset_library=True ) The above enr results are detailed in the GSEAPY documentation, and can be used to easily make pathway plots and perform network analysis.","title":"Pathway Analysis"},{"location":"usage/pathway_analysis/#example","text":"There is a jupyter notebook with some detailed examples of how to use this functionality and some possible plots. Pathway Enrichment : Demonstrates how to perform pathway enrichment analysis. Zhuoqing Fang, Xinyuan Liu, Gary Peltz, GSEApy: a comprehensive package for performing gene set enrichment analysis in Python, Bioinformatics, 2022;, btac757, https://doi.org/10.1093/bioinformatics/btac757 \u21a9","title":"Example"},{"location":"usage/quantification/","text":"Quantification Generally in bottom-up LC-MS/MS proteomics, you quantify precursors, which are broken up pieces of a protein (peptide) that contain some charge state. To make biological sense of quantified signal, it is useful to combine these precursors into their parent protein for downstream analysis. This process of protein quantification can be tricky, as there is no set standard that should always be used. Tip Our research tends to focus on using DIA-MS, and we restrict the spectral libraries used to analyze the data to proteotypic peptides, meaning that each precursor in the library is only linked to 1 protein. This makes protein quantification easier as no assumptions need to be made about precursors shared between proteins. DPKS provides 2 main protein quantification methods: Absolute Quantification : Using the top_n method. Relative Quantification : Using the maxlfq method. Absolute Quantification Absolute quantification is performed using the top_n method by combining a specified number of the most abundant precursors for each protein using a summarization method ( sum , mean , or median ). Note This is particularly useful if you want to compare proteins to other proteins in an experiment, like with a protein rank plot, to see what proteins are most abundant in your samples. Absolute quantification can be performed as follows: qm = qm.quantify( method=\"top_n\", top_n=3 ) The top_n parameter indicates how many of the precursors you want to use per protein for quantification. Relative Quantification DPKS uses the iq 1 implementation of the MaxLFQ algorithm 2 to extract optimal ratios between samples for each protein and combines them into a resulting protein quantity. Note Since this relative quantification approach uses all precursors for a protein, this is not suitable for protein rank plots, as certain proteins will have their absolute abundance underestimated. It is, however, considered state-of-the-art when measuring the differences in protein abundance between 2 experimental groups. Relative quantification can be performed as follows: qm = qm.quantify( method=\"maxlfq\", level=\"protein\", threads=5 ) Relative quantification takes much longer to process than absolute quantification, so we have optimized for performance using Numba for JIT compilation of the relative quantification algorithms. To enable multithreading, indicate the desired number of threads using the threads parameter. The level parameter indicates what level you want to quantify, whether it is proteins, peptides, or precursors. It is possible to quantify peptides using precursors, and precursors using fragment quantities (for DIA experiments) if the correct columns are supplied to the QuantMatrix . Combined Quantification To get the best of both worlds, a combined quantification approach is possible that applies relative quantification to the indicated top_n percursors for each protein. This allows for the overall abundance rank of the protein to remain intact while reaping the benefits of signal smoothing and ratio extraction used for relative quantification. Combined quantification can be performed simply by providing a value >0 to the top_n parameter of the quantify() method: qm = qm.quantify( method=\"maxlfq\", level=\"protein\", threads=5, top_n=3 ) Thang V Pham, Alex A Henneman, Connie R Jimenez, iq: an R package to estimate relative protein abundances from ion quantification in DIA-MS-based proteomics, Bioinformatics, Volume 36, Issue 8, April 2020, Pages 2611\u20132613, https://doi.org/10.1093/bioinformatics/btz961 \u21a9 Cox, J\u00fcrgen et al. Accurate Proteome-wide Label-free Quantification by Delayed Normalization and Maximal Peptide Ratio Extraction, Termed MaxLFQ. Molecular & Cellular Proteomics, Volume 13, Issue 9, 2513 - 2526, https://doi.org/10.1074/mcp.M113.031591 \u21a9","title":"Quantification"},{"location":"usage/quantification/#quantification","text":"Generally in bottom-up LC-MS/MS proteomics, you quantify precursors, which are broken up pieces of a protein (peptide) that contain some charge state. To make biological sense of quantified signal, it is useful to combine these precursors into their parent protein for downstream analysis. This process of protein quantification can be tricky, as there is no set standard that should always be used. Tip Our research tends to focus on using DIA-MS, and we restrict the spectral libraries used to analyze the data to proteotypic peptides, meaning that each precursor in the library is only linked to 1 protein. This makes protein quantification easier as no assumptions need to be made about precursors shared between proteins. DPKS provides 2 main protein quantification methods: Absolute Quantification : Using the top_n method. Relative Quantification : Using the maxlfq method.","title":"Quantification"},{"location":"usage/quantification/#absolute-quantification","text":"Absolute quantification is performed using the top_n method by combining a specified number of the most abundant precursors for each protein using a summarization method ( sum , mean , or median ). Note This is particularly useful if you want to compare proteins to other proteins in an experiment, like with a protein rank plot, to see what proteins are most abundant in your samples. Absolute quantification can be performed as follows: qm = qm.quantify( method=\"top_n\", top_n=3 ) The top_n parameter indicates how many of the precursors you want to use per protein for quantification.","title":"Absolute Quantification"},{"location":"usage/quantification/#relative-quantification","text":"DPKS uses the iq 1 implementation of the MaxLFQ algorithm 2 to extract optimal ratios between samples for each protein and combines them into a resulting protein quantity. Note Since this relative quantification approach uses all precursors for a protein, this is not suitable for protein rank plots, as certain proteins will have their absolute abundance underestimated. It is, however, considered state-of-the-art when measuring the differences in protein abundance between 2 experimental groups. Relative quantification can be performed as follows: qm = qm.quantify( method=\"maxlfq\", level=\"protein\", threads=5 ) Relative quantification takes much longer to process than absolute quantification, so we have optimized for performance using Numba for JIT compilation of the relative quantification algorithms. To enable multithreading, indicate the desired number of threads using the threads parameter. The level parameter indicates what level you want to quantify, whether it is proteins, peptides, or precursors. It is possible to quantify peptides using precursors, and precursors using fragment quantities (for DIA experiments) if the correct columns are supplied to the QuantMatrix .","title":"Relative Quantification"},{"location":"usage/quantification/#combined-quantification","text":"To get the best of both worlds, a combined quantification approach is possible that applies relative quantification to the indicated top_n percursors for each protein. This allows for the overall abundance rank of the protein to remain intact while reaping the benefits of signal smoothing and ratio extraction used for relative quantification. Combined quantification can be performed simply by providing a value >0 to the top_n parameter of the quantify() method: qm = qm.quantify( method=\"maxlfq\", level=\"protein\", threads=5, top_n=3 ) Thang V Pham, Alex A Henneman, Connie R Jimenez, iq: an R package to estimate relative protein abundances from ion quantification in DIA-MS-based proteomics, Bioinformatics, Volume 36, Issue 8, April 2020, Pages 2611\u20132613, https://doi.org/10.1093/bioinformatics/btz961 \u21a9 Cox, J\u00fcrgen et al. Accurate Proteome-wide Label-free Quantification by Delayed Normalization and Maximal Peptide Ratio Extraction, Termed MaxLFQ. Molecular & Cellular Proteomics, Volume 13, Issue 9, 2513 - 2526, https://doi.org/10.1074/mcp.M113.031591 \u21a9","title":"Combined Quantification"},{"location":"usage/statistical_comparisons/","text":"Statistical Comparisons DPKS allows for a number of different statistical tests to be performed between experimental groups in your data. Currently, it is possible to compare your samples using: T-test - 2-sided t-test for the independent samples. Linear Regression - 2-sided test to calculate a linear least-squares regression for the abundances betweeen experimental groups. ANOVA - One-way ANOVA to compare the means of 2 groups. Paired T-test - 2-sided t-test for 2 related samples (need to indicate a Pairs column in the Design Matrix). Note The scipy implementations of the above methods are currently used, but it is easy to add new comparison methods if a desired one is not yet available. T-test qm = qm.compare( method=\"ttest\", comparisons=(2,1), min_samples_per_group=10, level=\"protein\", multiple_testing_correction_method=\"fdr_tsbh\" ) Linear Regression qm = qm.compare( method=\"linregress\", comparisons=(2,1), min_samples_per_group=10, level=\"protein\", multiple_testing_correction_method=\"fdr_tsbh\" ) ANOVA qm = qm.compare( method=\"anova\", comparisons=(2,1), min_samples_per_group=10, level=\"protein\", multiple_testing_correction_method=\"fdr_tsbh\" ) Paired T-test In order to perform paired t-tests with your data, you first need to pass in a \"Pair\" column with your design matrix: Sample Group Pair s1 1 s2 s2 2 s1 s3 2 s4 s4 1 s3 Here, you need to make sure that your Sample is paired with another valid Sample in the list, but each sample pairing should be unique. qm = qm.compare( method=\"ttest_paired\", comparisons=(2,1), min_samples_per_group=10, level=\"protein\", multiple_testing_correction_method=\"fdr_tsbh\" ) Multiple Comparisons It is possible to perform multiple comparisons if you have multiple groups in your data by passing a list of tuples in as the comparison parameter: qm = qm.compare( method=\"ttest_paired\", comparisons=[(2,1), (3, 1), (4, 1)], min_samples_per_group=10, level=\"protein\", multiple_testing_correction_method=\"fdr_tsbh\" ) The above will separately compare groups 2, 3, and 4 to group 1 and write results columns for each of the 3 different comparisons. Example There is a jupyter notebook with some examples of how to use this functionality and some possible plots. Differential Expression : Demonstrates how to compute differences between two experimental conditions.","title":"Statistical Comparisons"},{"location":"usage/statistical_comparisons/#statistical-comparisons","text":"DPKS allows for a number of different statistical tests to be performed between experimental groups in your data. Currently, it is possible to compare your samples using: T-test - 2-sided t-test for the independent samples. Linear Regression - 2-sided test to calculate a linear least-squares regression for the abundances betweeen experimental groups. ANOVA - One-way ANOVA to compare the means of 2 groups. Paired T-test - 2-sided t-test for 2 related samples (need to indicate a Pairs column in the Design Matrix). Note The scipy implementations of the above methods are currently used, but it is easy to add new comparison methods if a desired one is not yet available.","title":"Statistical Comparisons"},{"location":"usage/statistical_comparisons/#t-test","text":"qm = qm.compare( method=\"ttest\", comparisons=(2,1), min_samples_per_group=10, level=\"protein\", multiple_testing_correction_method=\"fdr_tsbh\" )","title":"T-test"},{"location":"usage/statistical_comparisons/#linear-regression","text":"qm = qm.compare( method=\"linregress\", comparisons=(2,1), min_samples_per_group=10, level=\"protein\", multiple_testing_correction_method=\"fdr_tsbh\" )","title":"Linear Regression"},{"location":"usage/statistical_comparisons/#anova","text":"qm = qm.compare( method=\"anova\", comparisons=(2,1), min_samples_per_group=10, level=\"protein\", multiple_testing_correction_method=\"fdr_tsbh\" )","title":"ANOVA"},{"location":"usage/statistical_comparisons/#paired-t-test","text":"In order to perform paired t-tests with your data, you first need to pass in a \"Pair\" column with your design matrix: Sample Group Pair s1 1 s2 s2 2 s1 s3 2 s4 s4 1 s3 Here, you need to make sure that your Sample is paired with another valid Sample in the list, but each sample pairing should be unique. qm = qm.compare( method=\"ttest_paired\", comparisons=(2,1), min_samples_per_group=10, level=\"protein\", multiple_testing_correction_method=\"fdr_tsbh\" )","title":"Paired T-test"},{"location":"usage/statistical_comparisons/#multiple-comparisons","text":"It is possible to perform multiple comparisons if you have multiple groups in your data by passing a list of tuples in as the comparison parameter: qm = qm.compare( method=\"ttest_paired\", comparisons=[(2,1), (3, 1), (4, 1)], min_samples_per_group=10, level=\"protein\", multiple_testing_correction_method=\"fdr_tsbh\" ) The above will separately compare groups 2, 3, and 4 to group 1 and write results columns for each of the 3 different comparisons.","title":"Multiple Comparisons"},{"location":"usage/statistical_comparisons/#example","text":"There is a jupyter notebook with some examples of how to use this functionality and some possible plots. Differential Expression : Demonstrates how to compute differences between two experimental conditions.","title":"Example"}]}